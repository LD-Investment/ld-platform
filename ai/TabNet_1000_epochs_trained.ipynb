{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-01-01 00:00:00\n",
      "2017-09-28 02:01:00\n",
      "2017-11-08 18:01:00\n",
      "2017-12-20 10:01:00\n",
      "2018-01-31 03:01:00\n",
      "2018-03-15 03:01:00\n",
      "2018-04-25 19:01:00\n",
      "2018-06-06 11:01:00\n",
      "2018-07-18 21:01:00\n",
      "2018-08-29 13:01:00\n",
      "2018-10-10 05:01:00\n",
      "2018-11-21 07:01:00\n",
      "2019-01-01 23:01:00\n",
      "2019-02-12 15:01:00\n",
      "2019-03-26 13:01:00\n",
      "2019-05-07 05:01:00\n",
      "2019-06-18 07:01:00\n",
      "2019-07-29 23:01:00\n",
      "2019-09-09 23:01:00\n",
      "2019-10-21 15:01:00\n",
      "2019-12-02 11:01:00\n",
      "2020-01-13 03:01:00\n",
      "2020-02-24 01:01:00\n",
      "2020-04-05 18:01:00\n",
      "2020-05-17 12:01:00\n",
      "2020-06-28 07:01:00\n",
      "2020-08-08 23:01:00\n",
      "2020-09-19 15:01:00\n",
      "2020-10-31 07:01:00\n",
      "2020-12-12 00:01:00\n",
      "2021-01-22 20:01:00\n",
      "2021-03-05 13:01:00\n",
      "2021-04-16 06:01:00\n",
      "2021-05-28 03:01:00\n",
      "2021-07-08 19:01:00\n",
      "2021-08-19 15:01:00\n",
      "2021-09-30 09:01:00\n",
      "2021-11-11 01:01:00\n",
      "2021-12-22 17:01:00\n",
      "2021-12-31 23:01:00\n"
     ]
    }
   ],
   "source": [
    "COLUMNS = ['Open_time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close_time', 'quote_av', 'trades', \n",
    "                   'tb_base_av', 'tb_quote_av', 'ignore']\n",
    "URL = 'https://api.binance.com/api/v3/klines'\n",
    "def get_data(start_date, end_date, symbol):\n",
    "    data = []\n",
    "    \n",
    "    start = int(time.mktime(datetime.strptime(start_date + ' 00:00', '%Y-%m-%d %H:%M').timetuple())) * 1000\n",
    "    end = int(time.mktime(datetime.strptime(end_date +' 23:59', '%Y-%m-%d %H:%M').timetuple())) * 1000\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'interval': '1h',\n",
    "        'limit': 1000,\n",
    "        'startTime': start,\n",
    "        'endTime': end\n",
    "    }\n",
    "    \n",
    "    while start < end:\n",
    "        print(datetime.fromtimestamp(start // 1000))\n",
    "        params['startTime'] = start\n",
    "        result = requests.get(URL, params = params)\n",
    "        js = result.json()\n",
    "        if not js:\n",
    "            break\n",
    "        data.extend(js)  # result에 저장\n",
    "        start = js[-1][0] + 60000  # 다음 step으로\n",
    "    # 전처리\n",
    "    if not data:  # 해당 기간에 데이터가 없는 경우\n",
    "        print('해당 기간에 일치하는 데이터가 없습니다.')\n",
    "        return -1\n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = COLUMNS\n",
    "    df['Open_time'] = df.apply(lambda x:datetime.fromtimestamp(x['Open_time'] // 1000), axis=1)\n",
    "    df = df.drop(columns = ['Close_time', 'ignore'])\n",
    "    df['Symbol'] = symbol\n",
    "    df.loc[:, 'Open':'tb_quote_av'] = df.loc[:, 'Open':'tb_quote_av'].astype(float)  # string to float\n",
    "    df['trades'] = df['trades'].astype(int)\n",
    "    return df\n",
    "\n",
    "start_date = '2017-01-01'\n",
    "end_date = '2021-12-31'\n",
    "symbol = \"BTCUSDT\"\n",
    "train_df = get_data(start_date, end_date, symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "      <td>4261.48</td>\n",
       "      <td>4313.62</td>\n",
       "      <td>4261.32</td>\n",
       "      <td>4308.83</td>\n",
       "      <td>47.181009</td>\n",
       "      <td>2.023661e+05</td>\n",
       "      <td>171</td>\n",
       "      <td>35.160503</td>\n",
       "      <td>1.509525e+05</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-17 05:00:00</td>\n",
       "      <td>4308.83</td>\n",
       "      <td>4328.69</td>\n",
       "      <td>4291.37</td>\n",
       "      <td>4315.32</td>\n",
       "      <td>23.234916</td>\n",
       "      <td>1.003048e+05</td>\n",
       "      <td>102</td>\n",
       "      <td>21.448071</td>\n",
       "      <td>9.260828e+04</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-17 06:00:00</td>\n",
       "      <td>4330.29</td>\n",
       "      <td>4345.45</td>\n",
       "      <td>4309.37</td>\n",
       "      <td>4324.35</td>\n",
       "      <td>7.229691</td>\n",
       "      <td>3.128231e+04</td>\n",
       "      <td>36</td>\n",
       "      <td>4.802861</td>\n",
       "      <td>2.079532e+04</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-17 07:00:00</td>\n",
       "      <td>4316.62</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4287.41</td>\n",
       "      <td>4349.99</td>\n",
       "      <td>4.443249</td>\n",
       "      <td>1.924106e+04</td>\n",
       "      <td>25</td>\n",
       "      <td>2.602292</td>\n",
       "      <td>1.129135e+04</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-17 08:00:00</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4377.85</td>\n",
       "      <td>4333.32</td>\n",
       "      <td>4360.69</td>\n",
       "      <td>0.972807</td>\n",
       "      <td>4.239504e+03</td>\n",
       "      <td>28</td>\n",
       "      <td>0.814655</td>\n",
       "      <td>3.552747e+03</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38217</th>\n",
       "      <td>2021-12-31 19:00:00</td>\n",
       "      <td>46686.42</td>\n",
       "      <td>46688.68</td>\n",
       "      <td>45678.96</td>\n",
       "      <td>45728.28</td>\n",
       "      <td>3851.309020</td>\n",
       "      <td>1.776171e+08</td>\n",
       "      <td>87472</td>\n",
       "      <td>1691.645420</td>\n",
       "      <td>7.803749e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38218</th>\n",
       "      <td>2021-12-31 20:00:00</td>\n",
       "      <td>45728.28</td>\n",
       "      <td>46211.07</td>\n",
       "      <td>45678.00</td>\n",
       "      <td>45879.24</td>\n",
       "      <td>1831.412020</td>\n",
       "      <td>8.422142e+07</td>\n",
       "      <td>51300</td>\n",
       "      <td>997.723690</td>\n",
       "      <td>4.588270e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38219</th>\n",
       "      <td>2021-12-31 21:00:00</td>\n",
       "      <td>45886.66</td>\n",
       "      <td>46513.67</td>\n",
       "      <td>45766.85</td>\n",
       "      <td>46333.86</td>\n",
       "      <td>1734.695850</td>\n",
       "      <td>7.997065e+07</td>\n",
       "      <td>54579</td>\n",
       "      <td>972.440650</td>\n",
       "      <td>4.482430e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38220</th>\n",
       "      <td>2021-12-31 22:00:00</td>\n",
       "      <td>46333.87</td>\n",
       "      <td>46569.97</td>\n",
       "      <td>46232.36</td>\n",
       "      <td>46303.99</td>\n",
       "      <td>991.851420</td>\n",
       "      <td>4.600406e+07</td>\n",
       "      <td>36280</td>\n",
       "      <td>447.966050</td>\n",
       "      <td>2.077702e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38221</th>\n",
       "      <td>2021-12-31 23:00:00</td>\n",
       "      <td>46303.98</td>\n",
       "      <td>46520.13</td>\n",
       "      <td>46132.04</td>\n",
       "      <td>46216.93</td>\n",
       "      <td>985.997620</td>\n",
       "      <td>4.571976e+07</td>\n",
       "      <td>29555</td>\n",
       "      <td>484.092290</td>\n",
       "      <td>2.244919e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38222 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open_time      Open      High       Low     Close  \\\n",
       "0     2017-08-17 04:00:00   4261.48   4313.62   4261.32   4308.83   \n",
       "1     2017-08-17 05:00:00   4308.83   4328.69   4291.37   4315.32   \n",
       "2     2017-08-17 06:00:00   4330.29   4345.45   4309.37   4324.35   \n",
       "3     2017-08-17 07:00:00   4316.62   4349.99   4287.41   4349.99   \n",
       "4     2017-08-17 08:00:00   4333.32   4377.85   4333.32   4360.69   \n",
       "...                   ...       ...       ...       ...       ...   \n",
       "38217 2021-12-31 19:00:00  46686.42  46688.68  45678.96  45728.28   \n",
       "38218 2021-12-31 20:00:00  45728.28  46211.07  45678.00  45879.24   \n",
       "38219 2021-12-31 21:00:00  45886.66  46513.67  45766.85  46333.86   \n",
       "38220 2021-12-31 22:00:00  46333.87  46569.97  46232.36  46303.99   \n",
       "38221 2021-12-31 23:00:00  46303.98  46520.13  46132.04  46216.93   \n",
       "\n",
       "            Volume      quote_av  trades   tb_base_av   tb_quote_av   Symbol  \n",
       "0        47.181009  2.023661e+05     171    35.160503  1.509525e+05  BTCUSDT  \n",
       "1        23.234916  1.003048e+05     102    21.448071  9.260828e+04  BTCUSDT  \n",
       "2         7.229691  3.128231e+04      36     4.802861  2.079532e+04  BTCUSDT  \n",
       "3         4.443249  1.924106e+04      25     2.602292  1.129135e+04  BTCUSDT  \n",
       "4         0.972807  4.239504e+03      28     0.814655  3.552747e+03  BTCUSDT  \n",
       "...            ...           ...     ...          ...           ...      ...  \n",
       "38217  3851.309020  1.776171e+08   87472  1691.645420  7.803749e+07  BTCUSDT  \n",
       "38218  1831.412020  8.422142e+07   51300   997.723690  4.588270e+07  BTCUSDT  \n",
       "38219  1734.695850  7.997065e+07   54579   972.440650  4.482430e+07  BTCUSDT  \n",
       "38220   991.851420  4.600406e+07   36280   447.966050  2.077702e+07  BTCUSDT  \n",
       "38221   985.997620  4.571976e+07   29555   484.092290  2.244919e+07  BTCUSDT  \n",
       "\n",
       "[38222 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38222, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.dropna() \n",
    "\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 00:00:00\n",
      "2022-02-11 15:01:00\n",
      "2022-02-23 13:01:00\n"
     ]
    }
   ],
   "source": [
    "start_date = '2022-01-01'\n",
    "end_date = '2022-02-23'\n",
    "symbol = \"BTCUSDT\"\n",
    "test_df = get_data(start_date, end_date, symbol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open_time</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>quote_av</th>\n",
       "      <th>trades</th>\n",
       "      <th>tb_base_av</th>\n",
       "      <th>tb_quote_av</th>\n",
       "      <th>Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>46216.93</td>\n",
       "      <td>46731.39</td>\n",
       "      <td>46208.37</td>\n",
       "      <td>46656.13</td>\n",
       "      <td>1503.33095</td>\n",
       "      <td>6.987999e+07</td>\n",
       "      <td>38608</td>\n",
       "      <td>806.06536</td>\n",
       "      <td>3.746216e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>46656.14</td>\n",
       "      <td>46949.99</td>\n",
       "      <td>46574.06</td>\n",
       "      <td>46778.14</td>\n",
       "      <td>943.81539</td>\n",
       "      <td>4.412715e+07</td>\n",
       "      <td>31872</td>\n",
       "      <td>491.18067</td>\n",
       "      <td>2.296327e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-01 02:00:00</td>\n",
       "      <td>46778.14</td>\n",
       "      <td>46928.94</td>\n",
       "      <td>46721.96</td>\n",
       "      <td>46811.77</td>\n",
       "      <td>485.16860</td>\n",
       "      <td>2.272067e+07</td>\n",
       "      <td>24364</td>\n",
       "      <td>206.35113</td>\n",
       "      <td>9.662607e+06</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-01 03:00:00</td>\n",
       "      <td>46811.77</td>\n",
       "      <td>46916.63</td>\n",
       "      <td>46760.12</td>\n",
       "      <td>46813.20</td>\n",
       "      <td>562.88971</td>\n",
       "      <td>2.636326e+07</td>\n",
       "      <td>19882</td>\n",
       "      <td>248.28212</td>\n",
       "      <td>1.162938e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-01 04:00:00</td>\n",
       "      <td>46813.21</td>\n",
       "      <td>46887.33</td>\n",
       "      <td>46591.23</td>\n",
       "      <td>46711.05</td>\n",
       "      <td>861.88389</td>\n",
       "      <td>4.027204e+07</td>\n",
       "      <td>23357</td>\n",
       "      <td>397.53563</td>\n",
       "      <td>1.857540e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1281</th>\n",
       "      <td>2022-02-23 09:00:00</td>\n",
       "      <td>38673.02</td>\n",
       "      <td>38990.00</td>\n",
       "      <td>38600.01</td>\n",
       "      <td>38876.90</td>\n",
       "      <td>2085.38386</td>\n",
       "      <td>8.099305e+07</td>\n",
       "      <td>46811</td>\n",
       "      <td>1085.34474</td>\n",
       "      <td>4.215269e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1282</th>\n",
       "      <td>2022-02-23 10:00:00</td>\n",
       "      <td>38876.89</td>\n",
       "      <td>38956.64</td>\n",
       "      <td>38759.31</td>\n",
       "      <td>38866.27</td>\n",
       "      <td>1392.93540</td>\n",
       "      <td>5.414059e+07</td>\n",
       "      <td>34999</td>\n",
       "      <td>707.04677</td>\n",
       "      <td>2.748227e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>2022-02-23 11:00:00</td>\n",
       "      <td>38866.28</td>\n",
       "      <td>39125.47</td>\n",
       "      <td>38734.64</td>\n",
       "      <td>38782.91</td>\n",
       "      <td>1833.75028</td>\n",
       "      <td>7.130252e+07</td>\n",
       "      <td>41770</td>\n",
       "      <td>950.84726</td>\n",
       "      <td>3.697726e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1284</th>\n",
       "      <td>2022-02-23 12:00:00</td>\n",
       "      <td>38782.91</td>\n",
       "      <td>39100.00</td>\n",
       "      <td>38749.36</td>\n",
       "      <td>39016.39</td>\n",
       "      <td>1490.29190</td>\n",
       "      <td>5.798004e+07</td>\n",
       "      <td>38441</td>\n",
       "      <td>824.76875</td>\n",
       "      <td>3.208963e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>2022-02-23 13:00:00</td>\n",
       "      <td>39016.39</td>\n",
       "      <td>39249.93</td>\n",
       "      <td>38937.76</td>\n",
       "      <td>38965.46</td>\n",
       "      <td>1378.74684</td>\n",
       "      <td>5.387417e+07</td>\n",
       "      <td>32197</td>\n",
       "      <td>720.50886</td>\n",
       "      <td>2.815925e+07</td>\n",
       "      <td>BTCUSDT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1286 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open_time      Open      High       Low     Close      Volume  \\\n",
       "0    2022-01-01 00:00:00  46216.93  46731.39  46208.37  46656.13  1503.33095   \n",
       "1    2022-01-01 01:00:00  46656.14  46949.99  46574.06  46778.14   943.81539   \n",
       "2    2022-01-01 02:00:00  46778.14  46928.94  46721.96  46811.77   485.16860   \n",
       "3    2022-01-01 03:00:00  46811.77  46916.63  46760.12  46813.20   562.88971   \n",
       "4    2022-01-01 04:00:00  46813.21  46887.33  46591.23  46711.05   861.88389   \n",
       "...                  ...       ...       ...       ...       ...         ...   \n",
       "1281 2022-02-23 09:00:00  38673.02  38990.00  38600.01  38876.90  2085.38386   \n",
       "1282 2022-02-23 10:00:00  38876.89  38956.64  38759.31  38866.27  1392.93540   \n",
       "1283 2022-02-23 11:00:00  38866.28  39125.47  38734.64  38782.91  1833.75028   \n",
       "1284 2022-02-23 12:00:00  38782.91  39100.00  38749.36  39016.39  1490.29190   \n",
       "1285 2022-02-23 13:00:00  39016.39  39249.93  38937.76  38965.46  1378.74684   \n",
       "\n",
       "          quote_av  trades  tb_base_av   tb_quote_av   Symbol  \n",
       "0     6.987999e+07   38608   806.06536  3.746216e+07  BTCUSDT  \n",
       "1     4.412715e+07   31872   491.18067  2.296327e+07  BTCUSDT  \n",
       "2     2.272067e+07   24364   206.35113  9.662607e+06  BTCUSDT  \n",
       "3     2.636326e+07   19882   248.28212  1.162938e+07  BTCUSDT  \n",
       "4     4.027204e+07   23357   397.53563  1.857540e+07  BTCUSDT  \n",
       "...            ...     ...         ...           ...      ...  \n",
       "1281  8.099305e+07   46811  1085.34474  4.215269e+07  BTCUSDT  \n",
       "1282  5.414059e+07   34999   707.04677  2.748227e+07  BTCUSDT  \n",
       "1283  7.130252e+07   41770   950.84726  3.697726e+07  BTCUSDT  \n",
       "1284  5.798004e+07   38441   824.76875  3.208963e+07  BTCUSDT  \n",
       "1285  5.387417e+07   32197   720.50886  2.815925e+07  BTCUSDT  \n",
       "\n",
       "[1286 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"BTC_1hr_train.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"BTC_1hr_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"BTC_1hr_train.csv\") \n",
    "\n",
    "test_df = pd.read_csv(\"BTC_1hr_test.csv\") \n",
    "\n",
    "fear_greed = pd.read_csv(\"fear_greed_2022_02_23.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>공포지수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/02/23</td>\n",
       "      <td>36.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/02/22</td>\n",
       "      <td>33.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/02/21</td>\n",
       "      <td>26.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/02/20</td>\n",
       "      <td>32.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/02/19</td>\n",
       "      <td>38.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜   공포지수\n",
       "0  2022/02/23  36.91\n",
       "1  2022/02/22  33.21\n",
       "2  2022/02/21  26.89\n",
       "3  2022/02/20  32.28\n",
       "4  2022/02/19  38.83"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fear_greed = fear_greed[['날짜', '공포지수']] \n",
    "\n",
    "fear_greed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>날짜</th>\n",
       "      <th>공포지수</th>\n",
       "      <th>Days</th>\n",
       "      <th>Months</th>\n",
       "      <th>Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022/02/23</td>\n",
       "      <td>36.91</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022/02/22</td>\n",
       "      <td>33.21</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022/02/21</td>\n",
       "      <td>26.89</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022/02/20</td>\n",
       "      <td>32.28</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022/02/19</td>\n",
       "      <td>38.83</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           날짜   공포지수  Days  Months  Years\n",
       "0  2022/02/23  36.91    23       2   2022\n",
       "1  2022/02/22  33.21    22       2   2022\n",
       "2  2022/02/21  26.89    21       2   2022\n",
       "3  2022/02/20  32.28    20       2   2022\n",
       "4  2022/02/19  38.83    19       2   2022"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates = fear_greed['날짜'].values \n",
    "years, months, days = [], [], [] \n",
    "for date in dates:\n",
    "    splitted = date.split('/') \n",
    "    year = int(splitted[0]) \n",
    "    month = int(splitted[1]) \n",
    "    day = int(splitted[2]) \n",
    "    years.append(year) \n",
    "    months.append(month) \n",
    "    days.append(day) \n",
    "    \n",
    "fear_greed['Days'] = days \n",
    "fear_greed['Months'] = months \n",
    "fear_greed['Years'] = years \n",
    "\n",
    "fear_greed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC(df, n): \n",
    "    M = df.diff(n-1) \n",
    "    N = df.shift(n-1) \n",
    "    ROC = pd.Series(((M/N)*100), name = \"ROC_\" + str(n)) \n",
    "    return ROC \n",
    "\n",
    "def RSI(series, period): \n",
    "    delta = series.diff().dropna() \n",
    "    u = delta * 0 \n",
    "    d = u.copy() \n",
    "    u[delta > 0] = delta[delta > 0] \n",
    "    d[delta < 0] = -delta[delta < 0] \n",
    "    u[u.index[period-1]] = np.mean(u[:period]) # first value is sum of avg gains \n",
    "    u = u.drop(u.index[:(period-1)]) \n",
    "    d[d.index[period-1]] = np.mean(d[:period]) # first value is sum of avg losses \n",
    "    d = d.drop(d.index[:(period-1)])\n",
    "    rs = u.ewm(com=period-1, adjust=False).mean() / d.ewm(com=period-1, adjust=False).mean() \n",
    "    return 100 - 100 / (1+rs) \n",
    "\n",
    "def MOM(df, n): \n",
    "    MOM = pd.Series(df.diff(n), name=\"Momentum_\" + str(n)) \n",
    "    return MOM \n",
    "\n",
    "# stochastic oscillator %K\n",
    "def STOK(close, low, high, n): \n",
    "    stok = ((close - low.rolling(n).min()) / (high.rolling(n).max() - low.rolling(n).min())) * 100\n",
    "    return stok \n",
    "\n",
    "# stochatic oscillator %D\n",
    "def STOD(close, low, high, n):\n",
    "    stok = ((close - low.rolling(n).min()) / (high.rolling(n).max() - low.rolling(n).min())) * 100\n",
    "    stod = stok.rolling(3).mean() \n",
    "    return stod \n",
    "\n",
    "def feature_engineering(df): \n",
    "    print(\"===== Feature Engineering =====\")\n",
    "    windows = [5, 10, 14, 20, 30, 60, 120, 200] # from short term to long term \n",
    "    \n",
    "    # columns to drop \n",
    "    drop_cols = [\"Open_time\", \n",
    "                 \"Open\", \n",
    "                 \"High\", \n",
    "                 \"Low\", \n",
    "                 \"Close\", \n",
    "                 \"Volume\",\n",
    "                 \"quote_av\", \n",
    "                 \"trades\",\n",
    "                 \"tb_base_av\",\n",
    "                 \"tb_quote_av\", \n",
    "                 \"Symbol\"] \n",
    "    \n",
    "    for window in windows: \n",
    "        df['BTC_Open_ma{}'.format(window)] = df['Open'].rolling(window).mean() \n",
    "        df['BTC_Open_ma{}_ratio'.format(window)] = (df['Open'] - df['BTC_Open_ma{}'.format(window)]) / df['BTC_Open_ma{}'.format(window)]\n",
    "\n",
    "        df['BTC_High_ma{}'.format(window)] = df['High'].rolling(window).mean()\n",
    "        df['BTC_High_ma{}_ratio'.format(window)] = (df['High'] - df['BTC_High_ma{}'.format(window)]) / df['BTC_High_ma{}'.format(window)]\n",
    "\n",
    "        df['BTC_Low_ma{}'.format(window)] = df['Low'].rolling(window).mean() \n",
    "        df['BTC_Low_ma{}_ratio'.format(window)] = (df['Low'] - df['BTC_Low_ma{}'.format(window)]) / df['BTC_Low_ma{}'.format(window)]\n",
    "        \n",
    "        df['BTC_Close_ma{}'.format(window)] = df['Close'].rolling(window).mean() \n",
    "        df['BTC_Close_ma{}_ratio'.format(window)] = (df['Close'] - df['BTC_Close_ma{}'.format(window)]) / df['BTC_Close_ma{}'.format(window)]\n",
    "\n",
    "        df['BTC_Volume_ma{}'.format(window)] = df['Volume'].rolling(window).mean() \n",
    "        df['BTC_Volume_ma{}_ratio'.format(window)] = (df['Volume'] - df['BTC_Volume_ma{}'.format(window)]) / df['BTC_Volume_ma{}'.format(window)]\n",
    "        \n",
    "        df['BTC_quote_av_ma{}'.format(window)] = df['quote_av'].rolling(window).mean() \n",
    "        df['BTC_quote_av_ma{}_ratio'.format(window)] = (df['quote_av'] - df['BTC_quote_av_ma{}'.format(window)]) / df['BTC_quote_av_ma{}'.format(window)]\n",
    "        \n",
    "        df['BTC_trades_ma{}'.format(window)] = df['trades'].rolling(window).mean() \n",
    "        df['BTC_trades_ma{}_ratio'.format(window)] = (df['trades'] - df['BTC_trades_ma{}'.format(window)]) / df['BTC_trades_ma{}'.format(window)]\n",
    "        \n",
    "        df['BTC_tb_base_av_ma{}'.format(window)] = df['tb_base_av'].rolling(window).mean() \n",
    "        df['BTC_tb_base_av_ma{}_ratio'.format(window)] = (df['tb_base_av'] - df['BTC_tb_base_av_ma{}'.format(window)]) / df['BTC_tb_base_av_ma{}'.format(window)]\n",
    "        \n",
    "        df['BTC_tb_quote_av_ma{}'.format(window)] = df['tb_quote_av'].rolling(window).mean() \n",
    "        df['BTC_tb_quote_av_ma{}_ratio'.format(window)] = (df['tb_quote_av'] - df['BTC_tb_quote_av_ma{}'.format(window)]) / df['BTC_tb_quote_av_ma{}'.format(window)]\n",
    "        \n",
    "        df['ROC{}'.format(window)] = ROC(df['Open'], window)\n",
    "        \n",
    "        df['RSI{}'.format(window)] = RSI(df['Open'], window)\n",
    "        \n",
    "        df['MOM{}'.format(window)] = MOM(df['Open'], window) \n",
    "        \n",
    "        df['%K{}'.format(window)] = STOK(df['Close'], df['Low'], df['High'], window)\n",
    "        df['%D{}'.format(window)] = STOD(df['Close'], df['Low'], df['High'], window)\n",
    "        \n",
    "        drop_cols.append('BTC_Open_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_High_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_Low_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_Close_ma{}'.format(window)) \n",
    "        drop_cols.append('BTC_Volume_ma{}'.format(window)) \n",
    "        drop_cols.append('BTC_quote_av_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_trades_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_tb_base_av_ma{}'.format(window))\n",
    "        drop_cols.append('BTC_tb_quote_av_ma{}'.format(window))\n",
    "        \n",
    "    # extract date and time information \n",
    "    print(\"Extracting Date and Time Information...\")\n",
    "    hours, days, months, years = [], [], [], []  \n",
    "    future_hours, future_days, future_months = [], [], [] \n",
    "    for dt in tqdm(df['Open_time']):\n",
    "        hour = pd.to_datetime(dt).hour \n",
    "        day = pd.to_datetime(dt).day \n",
    "        month = pd.to_datetime(dt).month  \n",
    "        year = pd.to_datetime(dt).year \n",
    "        hours.append(hour) \n",
    "        days.append(day) \n",
    "        months.append(month)  \n",
    "        years.append(year)\n",
    "        \n",
    "        forecast_dt = pd.to_datetime(dt) + timedelta(hours=1) \n",
    "        future_hour = pd.to_datetime(forecast_dt).hour \n",
    "        future_day = pd.to_datetime(forecast_dt).day \n",
    "        future_month = pd.to_datetime(forecast_dt).month \n",
    "        future_hours.append(future_hour) \n",
    "        future_days.append(future_day) \n",
    "        future_months.append(future_month)\n",
    "        \n",
    "    df['Hours'] = hours \n",
    "    df['Days'] = days \n",
    "    df['Months'] = months \n",
    "    df['Years'] = years  \n",
    "    \n",
    "    df['Future_Hours'] = future_hours\n",
    "    df['Future_Days'] = future_days \n",
    "    df['Future_Months'] = future_months\n",
    "    \n",
    "    print(\"Creating Target Column...\") \n",
    "    targets = [None]\n",
    "    open_values = df['Open'].values \n",
    "    for i in range(1, open_values.shape[0]):\n",
    "        ret = (open_values[i] - open_values[i-1]) / open_values[i-1] * 100 \n",
    "        if ret >= 1.0:\n",
    "            targets.append(0) # long \n",
    "        elif ret <= -1.0:\n",
    "            targets.append(1) # short \n",
    "        else:\n",
    "            targets.append(2) # do nothing \n",
    "    df['Target'] = targets\n",
    "    \n",
    "        \n",
    "    \n",
    "    df = df.dropna() \n",
    "    # join fear greed index with df \n",
    "    df = df.merge(fear_greed, how='inner', left_on=['Days','Months','Years'], right_on=['Days','Months','Years'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    drop_cols.append('Years') \n",
    "    drop_cols.append('날짜') \n",
    "    df = df.rename(columns={'공포지수':'Fear_Greed_Index'}) \n",
    "    df = df.drop(columns={drop_cols[i] for i in range(len(drop_cols))})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Feature Engineering =====\n",
      "Extracting Date and Time Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38222/38222 [00:23<00:00, 1654.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Target Column...\n"
     ]
    }
   ],
   "source": [
    "processed_train = feature_engineering(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC_Open_ma5_ratio</th>\n",
       "      <th>BTC_High_ma5_ratio</th>\n",
       "      <th>BTC_Low_ma5_ratio</th>\n",
       "      <th>BTC_Close_ma5_ratio</th>\n",
       "      <th>BTC_Volume_ma5_ratio</th>\n",
       "      <th>BTC_quote_av_ma5_ratio</th>\n",
       "      <th>BTC_trades_ma5_ratio</th>\n",
       "      <th>BTC_tb_base_av_ma5_ratio</th>\n",
       "      <th>BTC_tb_quote_av_ma5_ratio</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>...</th>\n",
       "      <th>%K200</th>\n",
       "      <th>%D200</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Days</th>\n",
       "      <th>Months</th>\n",
       "      <th>Future_Hours</th>\n",
       "      <th>Future_Days</th>\n",
       "      <th>Future_Months</th>\n",
       "      <th>Target</th>\n",
       "      <th>Fear_Greed_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.010278</td>\n",
       "      <td>0.008660</td>\n",
       "      <td>0.403741</td>\n",
       "      <td>0.409748</td>\n",
       "      <td>0.708562</td>\n",
       "      <td>0.824174</td>\n",
       "      <td>0.831485</td>\n",
       "      <td>1.906163</td>\n",
       "      <td>...</td>\n",
       "      <td>75.765273</td>\n",
       "      <td>75.186343</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009489</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>-0.063745</td>\n",
       "      <td>-0.060823</td>\n",
       "      <td>-0.107997</td>\n",
       "      <td>-0.778443</td>\n",
       "      <td>-0.777619</td>\n",
       "      <td>2.468443</td>\n",
       "      <td>...</td>\n",
       "      <td>82.276063</td>\n",
       "      <td>75.981648</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008828</td>\n",
       "      <td>-0.002128</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>-0.348456</td>\n",
       "      <td>-0.347972</td>\n",
       "      <td>0.177334</td>\n",
       "      <td>-0.545114</td>\n",
       "      <td>-0.544360</td>\n",
       "      <td>2.296519</td>\n",
       "      <td>...</td>\n",
       "      <td>76.466502</td>\n",
       "      <td>78.169279</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000251</td>\n",
       "      <td>-0.002338</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>0.178103</td>\n",
       "      <td>0.072607</td>\n",
       "      <td>0.075098</td>\n",
       "      <td>-0.152018</td>\n",
       "      <td>...</td>\n",
       "      <td>77.172267</td>\n",
       "      <td>78.638277</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004500</td>\n",
       "      <td>-0.002717</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>-0.002992</td>\n",
       "      <td>-0.401288</td>\n",
       "      <td>-0.402381</td>\n",
       "      <td>-0.440659</td>\n",
       "      <td>-0.589132</td>\n",
       "      <td>-0.590297</td>\n",
       "      <td>1.197740</td>\n",
       "      <td>...</td>\n",
       "      <td>74.082040</td>\n",
       "      <td>75.906936</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>55.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BTC_Open_ma5_ratio  BTC_High_ma5_ratio  BTC_Low_ma5_ratio  \\\n",
       "0            0.007842            0.017503           0.010278   \n",
       "1            0.009489            0.006468           0.008016   \n",
       "2            0.008828           -0.002128           0.003764   \n",
       "3           -0.000251           -0.002338           0.003651   \n",
       "4            0.004500           -0.002717           0.000234   \n",
       "\n",
       "   BTC_Close_ma5_ratio  BTC_Volume_ma5_ratio  BTC_quote_av_ma5_ratio  \\\n",
       "0             0.008660              0.403741                0.409748   \n",
       "1             0.009009             -0.063745               -0.060823   \n",
       "2            -0.000407             -0.348456               -0.347972   \n",
       "3             0.001090              0.002688                0.004078   \n",
       "4            -0.002992             -0.401288               -0.402381   \n",
       "\n",
       "   BTC_trades_ma5_ratio  BTC_tb_base_av_ma5_ratio  BTC_tb_quote_av_ma5_ratio  \\\n",
       "0              0.708562                  0.824174                   0.831485   \n",
       "1             -0.107997                 -0.778443                  -0.777619   \n",
       "2              0.177334                 -0.545114                  -0.544360   \n",
       "3              0.178103                  0.072607                   0.075098   \n",
       "4             -0.440659                 -0.589132                  -0.590297   \n",
       "\n",
       "       ROC5  ...      %K200      %D200  Hours  Days  Months  Future_Hours  \\\n",
       "0  1.906163  ...  75.765273  75.186343      0     8      10             1   \n",
       "1  2.468443  ...  82.276063  75.981648      1     8      10             2   \n",
       "2  2.296519  ...  76.466502  78.169279      2     8      10             3   \n",
       "3 -0.152018  ...  77.172267  78.638277      3     8      10             4   \n",
       "4  1.197740  ...  74.082040  75.906936      4     8      10             5   \n",
       "\n",
       "   Future_Days  Future_Months  Target  Fear_Greed_Index  \n",
       "0            8             10     2.0             55.78  \n",
       "1            8             10     2.0             55.78  \n",
       "2            8             10     2.0             55.78  \n",
       "3            8             10     2.0             55.78  \n",
       "4            8             10     2.0             55.78  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Feature Engineering =====\n",
      "Extracting Date and Time Information...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1286/1286 [00:00<00:00, 1614.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Target Column...\n"
     ]
    }
   ],
   "source": [
    "processed_test = feature_engineering(test_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTC_Open_ma5_ratio</th>\n",
       "      <th>BTC_High_ma5_ratio</th>\n",
       "      <th>BTC_Low_ma5_ratio</th>\n",
       "      <th>BTC_Close_ma5_ratio</th>\n",
       "      <th>BTC_Volume_ma5_ratio</th>\n",
       "      <th>BTC_quote_av_ma5_ratio</th>\n",
       "      <th>BTC_trades_ma5_ratio</th>\n",
       "      <th>BTC_tb_base_av_ma5_ratio</th>\n",
       "      <th>BTC_tb_quote_av_ma5_ratio</th>\n",
       "      <th>ROC5</th>\n",
       "      <th>...</th>\n",
       "      <th>%K200</th>\n",
       "      <th>%D200</th>\n",
       "      <th>Hours</th>\n",
       "      <th>Days</th>\n",
       "      <th>Months</th>\n",
       "      <th>Future_Hours</th>\n",
       "      <th>Future_Days</th>\n",
       "      <th>Future_Months</th>\n",
       "      <th>Target</th>\n",
       "      <th>Fear_Greed_Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.004034</td>\n",
       "      <td>-0.003993</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>-0.005070</td>\n",
       "      <td>0.330785</td>\n",
       "      <td>0.325823</td>\n",
       "      <td>0.143445</td>\n",
       "      <td>0.365949</td>\n",
       "      <td>0.361019</td>\n",
       "      <td>-0.980860</td>\n",
       "      <td>...</td>\n",
       "      <td>13.473227</td>\n",
       "      <td>15.423466</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.005070</td>\n",
       "      <td>-0.003149</td>\n",
       "      <td>-0.004356</td>\n",
       "      <td>-0.001262</td>\n",
       "      <td>0.057771</td>\n",
       "      <td>0.054231</td>\n",
       "      <td>0.055382</td>\n",
       "      <td>0.032157</td>\n",
       "      <td>0.028586</td>\n",
       "      <td>-0.612677</td>\n",
       "      <td>...</td>\n",
       "      <td>15.271732</td>\n",
       "      <td>14.770018</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001261</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.102687</td>\n",
       "      <td>0.106246</td>\n",
       "      <td>0.024310</td>\n",
       "      <td>0.259915</td>\n",
       "      <td>0.263753</td>\n",
       "      <td>-0.546693</td>\n",
       "      <td>...</td>\n",
       "      <td>18.114835</td>\n",
       "      <td>15.619931</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>-0.249650</td>\n",
       "      <td>-0.248339</td>\n",
       "      <td>-0.134395</td>\n",
       "      <td>-0.346480</td>\n",
       "      <td>-0.345427</td>\n",
       "      <td>0.158214</td>\n",
       "      <td>...</td>\n",
       "      <td>16.637468</td>\n",
       "      <td>16.674678</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.206601</td>\n",
       "      <td>0.206151</td>\n",
       "      <td>0.057410</td>\n",
       "      <td>0.063595</td>\n",
       "      <td>0.063137</td>\n",
       "      <td>0.192768</td>\n",
       "      <td>...</td>\n",
       "      <td>12.938977</td>\n",
       "      <td>15.897094</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BTC_Open_ma5_ratio  BTC_High_ma5_ratio  BTC_Low_ma5_ratio  \\\n",
       "0           -0.004034           -0.003993          -0.004367   \n",
       "1           -0.005070           -0.003149          -0.004356   \n",
       "2           -0.001261            0.002895           0.001421   \n",
       "3            0.003922            0.001233           0.002520   \n",
       "4            0.001482            0.000799          -0.001495   \n",
       "\n",
       "   BTC_Close_ma5_ratio  BTC_Volume_ma5_ratio  BTC_quote_av_ma5_ratio  \\\n",
       "0            -0.005070              0.330785                0.325823   \n",
       "1            -0.001262              0.057771                0.054231   \n",
       "2             0.003922              0.102687                0.106246   \n",
       "3             0.001482             -0.249650               -0.248339   \n",
       "4            -0.004223              0.206601                0.206151   \n",
       "\n",
       "   BTC_trades_ma5_ratio  BTC_tb_base_av_ma5_ratio  BTC_tb_quote_av_ma5_ratio  \\\n",
       "0              0.143445                  0.365949                   0.361019   \n",
       "1              0.055382                  0.032157                   0.028586   \n",
       "2              0.024310                  0.259915                   0.263753   \n",
       "3             -0.134395                 -0.346480                  -0.345427   \n",
       "4              0.057410                  0.063595                   0.063137   \n",
       "\n",
       "       ROC5  ...      %K200      %D200  Hours  Days  Months  Future_Hours  \\\n",
       "0 -0.980860  ...  13.473227  15.423466      9     9       1            10   \n",
       "1 -0.612677  ...  15.271732  14.770018     10     9       1            11   \n",
       "2 -0.546693  ...  18.114835  15.619931     11     9       1            12   \n",
       "3  0.158214  ...  16.637468  16.674678     12     9       1            13   \n",
       "4  0.192768  ...  12.938977  15.897094     13     9       1            14   \n",
       "\n",
       "   Future_Days  Future_Months  Target  Fear_Greed_Index  \n",
       "0            9              1     2.0             40.02  \n",
       "1            9              1     2.0             40.02  \n",
       "2            9              1     2.0             40.02  \n",
       "3            9              1     2.0             40.02  \n",
       "4            9              1     2.0             40.02  \n",
       "\n",
       "[5 rows x 120 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Target', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of labels \n",
    "\n",
    "sns.countplot(processed_train['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Target', ylabel='count'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(processed_test['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36985, 119), (36985, 1))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [] \n",
    "for col in processed_train.columns:\n",
    "    if col != 'Target':\n",
    "        features.append(col) \n",
    "        \n",
    "X = processed_train[features].values\n",
    "Y = processed_train['Target'].values\n",
    "\n",
    "Y = Y.reshape((-1,1))\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 0.60941 | val_0_accuracy: 0.85645 |  0:00:02s\n",
      "epoch 1  | loss: 0.41523 | val_0_accuracy: 0.85239 |  0:00:04s\n",
      "epoch 2  | loss: 0.35068 | val_0_accuracy: 0.85591 |  0:00:07s\n",
      "epoch 3  | loss: 0.32944 | val_0_accuracy: 0.85726 |  0:00:09s\n",
      "epoch 4  | loss: 0.3156  | val_0_accuracy: 0.85699 |  0:00:12s\n",
      "epoch 5  | loss: 0.30863 | val_0_accuracy: 0.85672 |  0:00:14s\n",
      "epoch 6  | loss: 0.30627 | val_0_accuracy: 0.85699 |  0:00:16s\n",
      "epoch 7  | loss: 0.30446 | val_0_accuracy: 0.85699 |  0:00:19s\n",
      "epoch 8  | loss: 0.30372 | val_0_accuracy: 0.85699 |  0:00:21s\n",
      "epoch 9  | loss: 0.30591 | val_0_accuracy: 0.85699 |  0:00:23s\n",
      "epoch 10 | loss: 0.30303 | val_0_accuracy: 0.85726 |  0:00:26s\n",
      "epoch 11 | loss: 0.30196 | val_0_accuracy: 0.85699 |  0:00:28s\n",
      "epoch 12 | loss: 0.29651 | val_0_accuracy: 0.85699 |  0:00:30s\n",
      "epoch 13 | loss: 0.2935  | val_0_accuracy: 0.85699 |  0:00:33s\n",
      "epoch 14 | loss: 0.2922  | val_0_accuracy: 0.8578  |  0:00:35s\n",
      "epoch 15 | loss: 0.28871 | val_0_accuracy: 0.8578  |  0:00:38s\n",
      "epoch 16 | loss: 0.28471 | val_0_accuracy: 0.85834 |  0:00:41s\n",
      "epoch 17 | loss: 0.28481 | val_0_accuracy: 0.85996 |  0:00:43s\n",
      "epoch 18 | loss: 0.28448 | val_0_accuracy: 0.8605  |  0:00:46s\n",
      "epoch 19 | loss: 0.28165 | val_0_accuracy: 0.86104 |  0:00:49s\n",
      "epoch 20 | loss: 0.27865 | val_0_accuracy: 0.86348 |  0:00:52s\n",
      "epoch 21 | loss: 0.27649 | val_0_accuracy: 0.86267 |  0:00:54s\n",
      "epoch 22 | loss: 0.27635 | val_0_accuracy: 0.8678  |  0:00:56s\n",
      "epoch 23 | loss: 0.27507 | val_0_accuracy: 0.87051 |  0:00:59s\n",
      "epoch 24 | loss: 0.27055 | val_0_accuracy: 0.8678  |  0:01:01s\n",
      "epoch 25 | loss: 0.27054 | val_0_accuracy: 0.87132 |  0:01:04s\n",
      "epoch 26 | loss: 0.26568 | val_0_accuracy: 0.88159 |  0:01:06s\n",
      "epoch 27 | loss: 0.26149 | val_0_accuracy: 0.88159 |  0:01:09s\n",
      "epoch 28 | loss: 0.25009 | val_0_accuracy: 0.88321 |  0:01:11s\n",
      "epoch 29 | loss: 0.2411  | val_0_accuracy: 0.89916 |  0:01:13s\n",
      "epoch 30 | loss: 0.24061 | val_0_accuracy: 0.89889 |  0:01:16s\n",
      "epoch 31 | loss: 0.23059 | val_0_accuracy: 0.89646 |  0:01:18s\n",
      "epoch 32 | loss: 0.23428 | val_0_accuracy: 0.90132 |  0:01:20s\n",
      "epoch 33 | loss: 0.21871 | val_0_accuracy: 0.90646 |  0:01:23s\n",
      "epoch 34 | loss: 0.21554 | val_0_accuracy: 0.91106 |  0:01:26s\n",
      "epoch 35 | loss: 0.20651 | val_0_accuracy: 0.91592 |  0:01:28s\n",
      "epoch 36 | loss: 0.20483 | val_0_accuracy: 0.91403 |  0:01:31s\n",
      "epoch 37 | loss: 0.20009 | val_0_accuracy: 0.91349 |  0:01:33s\n",
      "epoch 38 | loss: 0.19851 | val_0_accuracy: 0.91268 |  0:01:36s\n",
      "epoch 39 | loss: 0.19658 | val_0_accuracy: 0.91187 |  0:01:38s\n",
      "epoch 40 | loss: 0.19753 | val_0_accuracy: 0.92133 |  0:01:40s\n",
      "epoch 41 | loss: 0.19262 | val_0_accuracy: 0.92106 |  0:01:43s\n",
      "epoch 42 | loss: 0.19422 | val_0_accuracy: 0.92187 |  0:01:45s\n",
      "epoch 43 | loss: 0.18804 | val_0_accuracy: 0.91673 |  0:01:48s\n",
      "epoch 44 | loss: 0.18979 | val_0_accuracy: 0.91971 |  0:01:50s\n",
      "epoch 45 | loss: 0.18579 | val_0_accuracy: 0.92214 |  0:01:53s\n",
      "epoch 46 | loss: 0.182   | val_0_accuracy: 0.92322 |  0:01:55s\n",
      "epoch 47 | loss: 0.18311 | val_0_accuracy: 0.9143  |  0:01:58s\n",
      "epoch 48 | loss: 0.18232 | val_0_accuracy: 0.91917 |  0:02:00s\n",
      "epoch 49 | loss: 0.18234 | val_0_accuracy: 0.92079 |  0:02:03s\n",
      "epoch 50 | loss: 0.18079 | val_0_accuracy: 0.90998 |  0:02:05s\n",
      "epoch 51 | loss: 0.18193 | val_0_accuracy: 0.90646 |  0:02:08s\n",
      "epoch 52 | loss: 0.18805 | val_0_accuracy: 0.91673 |  0:02:10s\n",
      "epoch 53 | loss: 0.178   | val_0_accuracy: 0.92268 |  0:02:13s\n",
      "epoch 54 | loss: 0.18043 | val_0_accuracy: 0.9143  |  0:02:15s\n",
      "epoch 55 | loss: 0.18405 | val_0_accuracy: 0.91755 |  0:02:18s\n",
      "epoch 56 | loss: 0.17541 | val_0_accuracy: 0.92701 |  0:02:21s\n",
      "epoch 57 | loss: 0.17158 | val_0_accuracy: 0.92295 |  0:02:24s\n",
      "epoch 58 | loss: 0.17175 | val_0_accuracy: 0.92268 |  0:02:26s\n",
      "epoch 59 | loss: 0.17156 | val_0_accuracy: 0.92133 |  0:02:28s\n",
      "epoch 60 | loss: 0.17135 | val_0_accuracy: 0.92268 |  0:02:31s\n",
      "epoch 61 | loss: 0.16908 | val_0_accuracy: 0.92998 |  0:02:33s\n",
      "epoch 62 | loss: 0.1698  | val_0_accuracy: 0.92403 |  0:02:35s\n",
      "epoch 63 | loss: 0.17224 | val_0_accuracy: 0.92052 |  0:02:38s\n",
      "epoch 64 | loss: 0.16268 | val_0_accuracy: 0.92809 |  0:02:40s\n",
      "epoch 65 | loss: 0.16132 | val_0_accuracy: 0.92484 |  0:02:42s\n",
      "epoch 66 | loss: 0.16719 | val_0_accuracy: 0.9243  |  0:02:45s\n",
      "epoch 67 | loss: 0.16052 | val_0_accuracy: 0.93079 |  0:02:47s\n",
      "epoch 68 | loss: 0.15597 | val_0_accuracy: 0.91944 |  0:02:50s\n",
      "epoch 69 | loss: 0.15716 | val_0_accuracy: 0.92836 |  0:02:52s\n",
      "epoch 70 | loss: 0.1565  | val_0_accuracy: 0.93052 |  0:02:54s\n",
      "epoch 71 | loss: 0.16103 | val_0_accuracy: 0.92971 |  0:02:57s\n",
      "epoch 72 | loss: 0.16066 | val_0_accuracy: 0.93025 |  0:02:59s\n",
      "epoch 73 | loss: 0.15828 | val_0_accuracy: 0.93187 |  0:03:01s\n",
      "epoch 74 | loss: 0.1522  | val_0_accuracy: 0.92917 |  0:03:04s\n",
      "epoch 75 | loss: 0.15095 | val_0_accuracy: 0.9335  |  0:03:06s\n",
      "epoch 76 | loss: 0.1533  | val_0_accuracy: 0.92052 |  0:03:09s\n",
      "epoch 77 | loss: 0.15264 | val_0_accuracy: 0.92863 |  0:03:12s\n",
      "epoch 78 | loss: 0.14854 | val_0_accuracy: 0.9316  |  0:03:14s\n",
      "epoch 79 | loss: 0.14913 | val_0_accuracy: 0.93295 |  0:03:17s\n",
      "epoch 80 | loss: 0.14803 | val_0_accuracy: 0.93268 |  0:03:19s\n",
      "epoch 81 | loss: 0.14872 | val_0_accuracy: 0.93701 |  0:03:22s\n",
      "epoch 82 | loss: 0.14826 | val_0_accuracy: 0.93241 |  0:03:24s\n",
      "epoch 83 | loss: 0.14764 | val_0_accuracy: 0.92539 |  0:03:26s\n",
      "epoch 84 | loss: 0.14431 | val_0_accuracy: 0.92944 |  0:03:28s\n",
      "epoch 85 | loss: 0.14498 | val_0_accuracy: 0.93566 |  0:03:31s\n",
      "epoch 86 | loss: 0.16568 | val_0_accuracy: 0.92268 |  0:03:33s\n",
      "epoch 87 | loss: 0.16534 | val_0_accuracy: 0.93458 |  0:03:36s\n",
      "epoch 88 | loss: 0.15106 | val_0_accuracy: 0.93404 |  0:03:38s\n",
      "epoch 89 | loss: 0.14826 | val_0_accuracy: 0.93025 |  0:03:41s\n",
      "epoch 90 | loss: 0.14825 | val_0_accuracy: 0.92971 |  0:03:43s\n",
      "epoch 91 | loss: 0.14454 | val_0_accuracy: 0.93539 |  0:03:45s\n",
      "epoch 92 | loss: 0.14512 | val_0_accuracy: 0.93323 |  0:03:48s\n",
      "epoch 93 | loss: 0.14363 | val_0_accuracy: 0.93647 |  0:03:50s\n",
      "epoch 94 | loss: 0.14457 | val_0_accuracy: 0.92484 |  0:03:52s\n",
      "epoch 95 | loss: 0.13842 | val_0_accuracy: 0.93674 |  0:03:55s\n",
      "epoch 96 | loss: 0.13895 | val_0_accuracy: 0.93809 |  0:03:57s\n",
      "epoch 97 | loss: 0.13774 | val_0_accuracy: 0.9389  |  0:04:00s\n",
      "epoch 98 | loss: 0.1367  | val_0_accuracy: 0.92025 |  0:04:03s\n",
      "epoch 99 | loss: 0.13708 | val_0_accuracy: 0.93647 |  0:04:05s\n",
      "epoch 100| loss: 0.13682 | val_0_accuracy: 0.9362  |  0:04:07s\n",
      "epoch 101| loss: 0.13309 | val_0_accuracy: 0.92998 |  0:04:10s\n",
      "epoch 102| loss: 0.13785 | val_0_accuracy: 0.93944 |  0:04:12s\n",
      "epoch 103| loss: 0.13696 | val_0_accuracy: 0.94134 |  0:04:15s\n",
      "epoch 104| loss: 0.13404 | val_0_accuracy: 0.93728 |  0:04:17s\n",
      "epoch 105| loss: 0.14037 | val_0_accuracy: 0.93998 |  0:04:20s\n",
      "epoch 106| loss: 0.13907 | val_0_accuracy: 0.93187 |  0:04:22s\n",
      "epoch 107| loss: 0.13736 | val_0_accuracy: 0.93728 |  0:04:25s\n",
      "epoch 108| loss: 0.13545 | val_0_accuracy: 0.93566 |  0:04:27s\n",
      "epoch 109| loss: 0.13368 | val_0_accuracy: 0.93755 |  0:04:30s\n",
      "epoch 110| loss: 0.1309  | val_0_accuracy: 0.9362  |  0:04:32s\n",
      "epoch 111| loss: 0.13114 | val_0_accuracy: 0.93971 |  0:04:35s\n",
      "epoch 112| loss: 0.1261  | val_0_accuracy: 0.93431 |  0:04:37s\n",
      "epoch 113| loss: 0.12923 | val_0_accuracy: 0.93133 |  0:04:39s\n",
      "epoch 114| loss: 0.13478 | val_0_accuracy: 0.93755 |  0:04:41s\n",
      "epoch 115| loss: 0.12696 | val_0_accuracy: 0.93971 |  0:04:44s\n",
      "epoch 116| loss: 0.12761 | val_0_accuracy: 0.9435  |  0:04:46s\n",
      "epoch 117| loss: 0.13246 | val_0_accuracy: 0.92511 |  0:04:48s\n",
      "epoch 118| loss: 0.13016 | val_0_accuracy: 0.93052 |  0:04:51s\n",
      "epoch 119| loss: 0.12657 | val_0_accuracy: 0.93755 |  0:04:53s\n",
      "epoch 120| loss: 0.12595 | val_0_accuracy: 0.92998 |  0:04:56s\n",
      "epoch 121| loss: 0.12575 | val_0_accuracy: 0.9362  |  0:04:58s\n",
      "epoch 122| loss: 0.12451 | val_0_accuracy: 0.93755 |  0:05:00s\n",
      "epoch 123| loss: 0.12132 | val_0_accuracy: 0.93809 |  0:05:02s\n",
      "epoch 124| loss: 0.12994 | val_0_accuracy: 0.93944 |  0:05:05s\n",
      "epoch 125| loss: 0.12485 | val_0_accuracy: 0.92728 |  0:05:07s\n",
      "epoch 126| loss: 0.13275 | val_0_accuracy: 0.93593 |  0:05:09s\n",
      "epoch 127| loss: 0.12275 | val_0_accuracy: 0.93458 |  0:05:12s\n",
      "epoch 128| loss: 0.11854 | val_0_accuracy: 0.93187 |  0:05:14s\n",
      "epoch 129| loss: 0.11992 | val_0_accuracy: 0.9362  |  0:05:17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 130| loss: 0.11953 | val_0_accuracy: 0.93917 |  0:05:19s\n",
      "epoch 131| loss: 0.11915 | val_0_accuracy: 0.93133 |  0:05:22s\n",
      "epoch 132| loss: 0.12088 | val_0_accuracy: 0.93674 |  0:05:24s\n",
      "epoch 133| loss: 0.11973 | val_0_accuracy: 0.93755 |  0:05:26s\n",
      "epoch 134| loss: 0.11935 | val_0_accuracy: 0.93512 |  0:05:29s\n",
      "epoch 135| loss: 0.11945 | val_0_accuracy: 0.93836 |  0:05:31s\n",
      "epoch 136| loss: 0.11779 | val_0_accuracy: 0.93782 |  0:05:33s\n",
      "epoch 137| loss: 0.12432 | val_0_accuracy: 0.94052 |  0:05:36s\n",
      "epoch 138| loss: 0.11713 | val_0_accuracy: 0.93863 |  0:05:38s\n",
      "epoch 139| loss: 0.11597 | val_0_accuracy: 0.93728 |  0:05:41s\n",
      "epoch 140| loss: 0.11651 | val_0_accuracy: 0.93728 |  0:05:43s\n",
      "epoch 141| loss: 0.11778 | val_0_accuracy: 0.93755 |  0:05:45s\n",
      "epoch 142| loss: 0.11721 | val_0_accuracy: 0.93944 |  0:05:48s\n",
      "epoch 143| loss: 0.11683 | val_0_accuracy: 0.93566 |  0:05:50s\n",
      "epoch 144| loss: 0.11881 | val_0_accuracy: 0.93836 |  0:05:53s\n",
      "epoch 145| loss: 0.1161  | val_0_accuracy: 0.93944 |  0:05:55s\n",
      "epoch 146| loss: 0.11476 | val_0_accuracy: 0.9362  |  0:05:58s\n",
      "epoch 147| loss: 0.11595 | val_0_accuracy: 0.93728 |  0:06:00s\n",
      "epoch 148| loss: 0.11416 | val_0_accuracy: 0.93431 |  0:06:03s\n",
      "epoch 149| loss: 0.1154  | val_0_accuracy: 0.94107 |  0:06:05s\n",
      "epoch 150| loss: 0.11383 | val_0_accuracy: 0.93295 |  0:06:07s\n",
      "epoch 151| loss: 0.11273 | val_0_accuracy: 0.93971 |  0:06:10s\n",
      "epoch 152| loss: 0.11144 | val_0_accuracy: 0.93539 |  0:06:13s\n",
      "epoch 153| loss: 0.1168  | val_0_accuracy: 0.9362  |  0:06:15s\n",
      "epoch 154| loss: 0.11146 | val_0_accuracy: 0.93431 |  0:06:17s\n",
      "epoch 155| loss: 0.1123  | val_0_accuracy: 0.93187 |  0:06:20s\n",
      "epoch 156| loss: 0.11571 | val_0_accuracy: 0.9389  |  0:06:22s\n",
      "epoch 157| loss: 0.11189 | val_0_accuracy: 0.93917 |  0:06:25s\n",
      "epoch 158| loss: 0.11154 | val_0_accuracy: 0.93809 |  0:06:27s\n",
      "epoch 159| loss: 0.11409 | val_0_accuracy: 0.9362  |  0:06:29s\n",
      "epoch 160| loss: 0.10855 | val_0_accuracy: 0.93458 |  0:06:32s\n",
      "epoch 161| loss: 0.10954 | val_0_accuracy: 0.93647 |  0:06:34s\n",
      "epoch 162| loss: 0.112   | val_0_accuracy: 0.93944 |  0:06:37s\n",
      "epoch 163| loss: 0.11542 | val_0_accuracy: 0.93863 |  0:06:39s\n",
      "epoch 164| loss: 0.11462 | val_0_accuracy: 0.93836 |  0:06:41s\n",
      "epoch 165| loss: 0.10712 | val_0_accuracy: 0.93404 |  0:06:44s\n",
      "epoch 166| loss: 0.1145  | val_0_accuracy: 0.93593 |  0:06:46s\n",
      "epoch 167| loss: 0.10657 | val_0_accuracy: 0.93674 |  0:06:48s\n",
      "epoch 168| loss: 0.10797 | val_0_accuracy: 0.94107 |  0:06:51s\n",
      "epoch 169| loss: 0.10798 | val_0_accuracy: 0.93998 |  0:06:53s\n",
      "epoch 170| loss: 0.10875 | val_0_accuracy: 0.94242 |  0:06:56s\n",
      "epoch 171| loss: 0.10349 | val_0_accuracy: 0.93566 |  0:06:58s\n",
      "epoch 172| loss: 0.10947 | val_0_accuracy: 0.94107 |  0:07:00s\n",
      "epoch 173| loss: 0.10871 | val_0_accuracy: 0.93863 |  0:07:03s\n",
      "epoch 174| loss: 0.10907 | val_0_accuracy: 0.93917 |  0:07:05s\n",
      "epoch 175| loss: 0.1043  | val_0_accuracy: 0.93971 |  0:07:08s\n",
      "epoch 176| loss: 0.10985 | val_0_accuracy: 0.9289  |  0:07:10s\n",
      "epoch 177| loss: 0.11138 | val_0_accuracy: 0.93836 |  0:07:12s\n",
      "epoch 178| loss: 0.10808 | val_0_accuracy: 0.94161 |  0:07:15s\n",
      "epoch 179| loss: 0.10699 | val_0_accuracy: 0.93106 |  0:07:17s\n",
      "epoch 180| loss: 0.10963 | val_0_accuracy: 0.94404 |  0:07:19s\n",
      "epoch 181| loss: 0.11429 | val_0_accuracy: 0.93782 |  0:07:22s\n",
      "epoch 182| loss: 0.10724 | val_0_accuracy: 0.93728 |  0:07:24s\n",
      "epoch 183| loss: 0.10698 | val_0_accuracy: 0.9389  |  0:07:27s\n",
      "epoch 184| loss: 0.11187 | val_0_accuracy: 0.94107 |  0:07:30s\n",
      "epoch 185| loss: 0.10747 | val_0_accuracy: 0.93917 |  0:07:32s\n",
      "epoch 186| loss: 0.10375 | val_0_accuracy: 0.93539 |  0:07:35s\n",
      "epoch 187| loss: 0.11302 | val_0_accuracy: 0.93863 |  0:07:37s\n",
      "epoch 188| loss: 0.10519 | val_0_accuracy: 0.94242 |  0:07:40s\n",
      "epoch 189| loss: 0.11066 | val_0_accuracy: 0.93214 |  0:07:42s\n",
      "epoch 190| loss: 0.11708 | val_0_accuracy: 0.93971 |  0:07:45s\n",
      "epoch 191| loss: 0.10982 | val_0_accuracy: 0.94296 |  0:07:47s\n",
      "epoch 192| loss: 0.11157 | val_0_accuracy: 0.93106 |  0:07:50s\n",
      "epoch 193| loss: 0.10836 | val_0_accuracy: 0.9389  |  0:07:52s\n",
      "epoch 194| loss: 0.10429 | val_0_accuracy: 0.94215 |  0:07:54s\n",
      "epoch 195| loss: 0.10671 | val_0_accuracy: 0.94161 |  0:07:57s\n",
      "epoch 196| loss: 0.10817 | val_0_accuracy: 0.92701 |  0:07:59s\n",
      "epoch 197| loss: 0.10664 | val_0_accuracy: 0.93782 |  0:08:01s\n",
      "epoch 198| loss: 0.10006 | val_0_accuracy: 0.93755 |  0:08:04s\n",
      "epoch 199| loss: 0.10436 | val_0_accuracy: 0.93674 |  0:08:06s\n",
      "epoch 200| loss: 0.10253 | val_0_accuracy: 0.93728 |  0:08:08s\n",
      "epoch 201| loss: 0.10471 | val_0_accuracy: 0.93998 |  0:08:11s\n",
      "epoch 202| loss: 0.10442 | val_0_accuracy: 0.93431 |  0:08:13s\n",
      "epoch 203| loss: 0.10276 | val_0_accuracy: 0.93458 |  0:08:15s\n",
      "epoch 204| loss: 0.10383 | val_0_accuracy: 0.93566 |  0:08:18s\n",
      "epoch 205| loss: 0.10103 | val_0_accuracy: 0.93836 |  0:08:20s\n",
      "epoch 206| loss: 0.10161 | val_0_accuracy: 0.93539 |  0:08:22s\n",
      "epoch 207| loss: 0.10054 | val_0_accuracy: 0.94134 |  0:08:25s\n",
      "epoch 208| loss: 0.09942 | val_0_accuracy: 0.93728 |  0:08:28s\n",
      "epoch 209| loss: 0.10248 | val_0_accuracy: 0.94269 |  0:08:30s\n",
      "epoch 210| loss: 0.10099 | val_0_accuracy: 0.93971 |  0:08:33s\n",
      "epoch 211| loss: 0.10179 | val_0_accuracy: 0.93214 |  0:08:35s\n",
      "epoch 212| loss: 0.10891 | val_0_accuracy: 0.93998 |  0:08:38s\n",
      "epoch 213| loss: 0.09857 | val_0_accuracy: 0.93593 |  0:08:40s\n",
      "epoch 214| loss: 0.10261 | val_0_accuracy: 0.93782 |  0:08:42s\n",
      "epoch 215| loss: 0.10956 | val_0_accuracy: 0.94269 |  0:08:45s\n",
      "epoch 216| loss: 0.11002 | val_0_accuracy: 0.94052 |  0:08:47s\n",
      "epoch 217| loss: 0.10307 | val_0_accuracy: 0.93971 |  0:08:50s\n",
      "epoch 218| loss: 0.10145 | val_0_accuracy: 0.94242 |  0:08:52s\n",
      "epoch 219| loss: 0.09883 | val_0_accuracy: 0.93809 |  0:08:54s\n",
      "epoch 220| loss: 0.09916 | val_0_accuracy: 0.93133 |  0:08:57s\n",
      "epoch 221| loss: 0.10251 | val_0_accuracy: 0.93917 |  0:08:59s\n",
      "epoch 222| loss: 0.10251 | val_0_accuracy: 0.94161 |  0:09:01s\n",
      "epoch 223| loss: 0.10691 | val_0_accuracy: 0.93836 |  0:09:04s\n",
      "epoch 224| loss: 0.10134 | val_0_accuracy: 0.94215 |  0:09:06s\n",
      "epoch 225| loss: 0.10203 | val_0_accuracy: 0.93863 |  0:09:09s\n",
      "epoch 226| loss: 0.09984 | val_0_accuracy: 0.93485 |  0:09:12s\n",
      "epoch 227| loss: 0.10145 | val_0_accuracy: 0.93377 |  0:09:15s\n",
      "epoch 228| loss: 0.10567 | val_0_accuracy: 0.93863 |  0:09:17s\n",
      "epoch 229| loss: 0.09877 | val_0_accuracy: 0.94377 |  0:09:20s\n",
      "epoch 230| loss: 0.09266 | val_0_accuracy: 0.94215 |  0:09:22s\n",
      "epoch 231| loss: 0.10463 | val_0_accuracy: 0.93782 |  0:09:24s\n",
      "epoch 232| loss: 0.09891 | val_0_accuracy: 0.93593 |  0:09:27s\n",
      "epoch 233| loss: 0.10172 | val_0_accuracy: 0.94188 |  0:09:29s\n",
      "epoch 234| loss: 0.0992  | val_0_accuracy: 0.9389  |  0:09:31s\n",
      "epoch 235| loss: 0.09879 | val_0_accuracy: 0.9362  |  0:09:34s\n",
      "epoch 236| loss: 0.10529 | val_0_accuracy: 0.94025 |  0:09:36s\n",
      "epoch 237| loss: 0.1029  | val_0_accuracy: 0.93971 |  0:09:38s\n",
      "epoch 238| loss: 0.10035 | val_0_accuracy: 0.94269 |  0:09:41s\n",
      "epoch 239| loss: 0.09699 | val_0_accuracy: 0.93917 |  0:09:43s\n",
      "epoch 240| loss: 0.09965 | val_0_accuracy: 0.94269 |  0:09:45s\n",
      "epoch 241| loss: 0.09881 | val_0_accuracy: 0.94161 |  0:09:48s\n",
      "epoch 242| loss: 0.09629 | val_0_accuracy: 0.94025 |  0:09:50s\n",
      "epoch 243| loss: 0.09774 | val_0_accuracy: 0.93809 |  0:09:52s\n",
      "epoch 244| loss: 0.09532 | val_0_accuracy: 0.93593 |  0:09:55s\n",
      "epoch 245| loss: 0.10071 | val_0_accuracy: 0.93917 |  0:09:57s\n",
      "epoch 246| loss: 0.09648 | val_0_accuracy: 0.93241 |  0:09:59s\n",
      "epoch 247| loss: 0.09507 | val_0_accuracy: 0.93782 |  0:10:02s\n",
      "epoch 248| loss: 0.09915 | val_0_accuracy: 0.93755 |  0:10:04s\n",
      "epoch 249| loss: 0.10129 | val_0_accuracy: 0.93917 |  0:10:06s\n",
      "epoch 250| loss: 0.10027 | val_0_accuracy: 0.93944 |  0:10:09s\n",
      "epoch 251| loss: 0.09611 | val_0_accuracy: 0.93674 |  0:10:11s\n",
      "epoch 252| loss: 0.09377 | val_0_accuracy: 0.94134 |  0:10:14s\n",
      "epoch 253| loss: 0.09635 | val_0_accuracy: 0.93052 |  0:10:16s\n",
      "epoch 254| loss: 0.12134 | val_0_accuracy: 0.93404 |  0:10:19s\n",
      "epoch 255| loss: 0.10792 | val_0_accuracy: 0.93647 |  0:10:21s\n",
      "epoch 256| loss: 0.09926 | val_0_accuracy: 0.93809 |  0:10:24s\n",
      "epoch 257| loss: 0.09887 | val_0_accuracy: 0.94052 |  0:10:26s\n",
      "epoch 258| loss: 0.0962  | val_0_accuracy: 0.93998 |  0:10:29s\n",
      "epoch 259| loss: 0.09667 | val_0_accuracy: 0.94107 |  0:10:31s\n",
      "epoch 260| loss: 0.09334 | val_0_accuracy: 0.94161 |  0:10:34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 261| loss: 0.09237 | val_0_accuracy: 0.93512 |  0:10:36s\n",
      "epoch 262| loss: 0.09749 | val_0_accuracy: 0.94025 |  0:10:38s\n",
      "epoch 263| loss: 0.09244 | val_0_accuracy: 0.94079 |  0:10:41s\n",
      "epoch 264| loss: 0.09439 | val_0_accuracy: 0.93458 |  0:10:43s\n",
      "epoch 265| loss: 0.09602 | val_0_accuracy: 0.94079 |  0:10:45s\n",
      "epoch 266| loss: 0.09252 | val_0_accuracy: 0.9389  |  0:10:48s\n",
      "epoch 267| loss: 0.10137 | val_0_accuracy: 0.93512 |  0:10:50s\n",
      "epoch 268| loss: 0.09703 | val_0_accuracy: 0.93782 |  0:10:53s\n",
      "epoch 269| loss: 0.10153 | val_0_accuracy: 0.93566 |  0:10:55s\n",
      "epoch 270| loss: 0.09963 | val_0_accuracy: 0.93512 |  0:10:58s\n",
      "epoch 271| loss: 0.09489 | val_0_accuracy: 0.93701 |  0:11:00s\n",
      "epoch 272| loss: 0.09238 | val_0_accuracy: 0.9389  |  0:11:02s\n",
      "epoch 273| loss: 0.09767 | val_0_accuracy: 0.93728 |  0:11:05s\n",
      "epoch 274| loss: 0.09709 | val_0_accuracy: 0.94242 |  0:11:07s\n",
      "epoch 275| loss: 0.09256 | val_0_accuracy: 0.93214 |  0:11:09s\n",
      "epoch 276| loss: 0.09516 | val_0_accuracy: 0.93593 |  0:11:12s\n",
      "epoch 277| loss: 0.0954  | val_0_accuracy: 0.93998 |  0:11:14s\n",
      "epoch 278| loss: 0.09191 | val_0_accuracy: 0.93863 |  0:11:17s\n",
      "epoch 279| loss: 0.09263 | val_0_accuracy: 0.93863 |  0:11:19s\n",
      "epoch 280| loss: 0.09071 | val_0_accuracy: 0.93214 |  0:11:22s\n",
      "epoch 281| loss: 0.09405 | val_0_accuracy: 0.93917 |  0:11:24s\n",
      "epoch 282| loss: 0.09271 | val_0_accuracy: 0.93431 |  0:11:26s\n",
      "epoch 283| loss: 0.09127 | val_0_accuracy: 0.93566 |  0:11:29s\n",
      "epoch 284| loss: 0.09256 | val_0_accuracy: 0.93971 |  0:11:31s\n",
      "epoch 285| loss: 0.09446 | val_0_accuracy: 0.93917 |  0:11:34s\n",
      "epoch 286| loss: 0.09418 | val_0_accuracy: 0.93079 |  0:11:36s\n",
      "epoch 287| loss: 0.09047 | val_0_accuracy: 0.94052 |  0:11:39s\n",
      "epoch 288| loss: 0.0897  | val_0_accuracy: 0.94052 |  0:11:41s\n",
      "epoch 289| loss: 0.09786 | val_0_accuracy: 0.93052 |  0:11:43s\n",
      "epoch 290| loss: 0.10174 | val_0_accuracy: 0.94025 |  0:11:46s\n",
      "epoch 291| loss: 0.08637 | val_0_accuracy: 0.93701 |  0:11:48s\n",
      "epoch 292| loss: 0.0951  | val_0_accuracy: 0.93863 |  0:11:51s\n",
      "epoch 293| loss: 0.09133 | val_0_accuracy: 0.93647 |  0:11:53s\n",
      "epoch 294| loss: 0.09238 | val_0_accuracy: 0.94323 |  0:11:55s\n",
      "epoch 295| loss: 0.09208 | val_0_accuracy: 0.93809 |  0:11:58s\n",
      "epoch 296| loss: 0.08886 | val_0_accuracy: 0.93782 |  0:12:00s\n",
      "epoch 297| loss: 0.09109 | val_0_accuracy: 0.93323 |  0:12:02s\n",
      "epoch 298| loss: 0.09501 | val_0_accuracy: 0.94052 |  0:12:05s\n",
      "epoch 299| loss: 0.09274 | val_0_accuracy: 0.93701 |  0:12:07s\n",
      "epoch 300| loss: 0.09117 | val_0_accuracy: 0.9389  |  0:12:09s\n",
      "epoch 301| loss: 0.0917  | val_0_accuracy: 0.9362  |  0:12:12s\n",
      "epoch 302| loss: 0.09362 | val_0_accuracy: 0.93971 |  0:12:15s\n",
      "epoch 303| loss: 0.08915 | val_0_accuracy: 0.94161 |  0:12:17s\n",
      "epoch 304| loss: 0.09328 | val_0_accuracy: 0.93809 |  0:12:19s\n",
      "epoch 305| loss: 0.08941 | val_0_accuracy: 0.94079 |  0:12:22s\n",
      "epoch 306| loss: 0.09029 | val_0_accuracy: 0.9335  |  0:12:24s\n",
      "epoch 307| loss: 0.09231 | val_0_accuracy: 0.93782 |  0:12:27s\n",
      "epoch 308| loss: 0.09208 | val_0_accuracy: 0.93809 |  0:12:30s\n",
      "epoch 309| loss: 0.09196 | val_0_accuracy: 0.93971 |  0:12:33s\n",
      "epoch 310| loss: 0.09241 | val_0_accuracy: 0.94161 |  0:12:35s\n",
      "epoch 311| loss: 0.09467 | val_0_accuracy: 0.93701 |  0:12:38s\n",
      "epoch 312| loss: 0.08826 | val_0_accuracy: 0.93674 |  0:12:41s\n",
      "epoch 313| loss: 0.08855 | val_0_accuracy: 0.93863 |  0:12:44s\n",
      "epoch 314| loss: 0.09231 | val_0_accuracy: 0.93647 |  0:12:46s\n",
      "epoch 315| loss: 0.09083 | val_0_accuracy: 0.93214 |  0:12:48s\n",
      "epoch 316| loss: 0.09427 | val_0_accuracy: 0.93458 |  0:12:51s\n",
      "epoch 317| loss: 0.0919  | val_0_accuracy: 0.93782 |  0:12:53s\n",
      "epoch 318| loss: 0.08926 | val_0_accuracy: 0.94079 |  0:12:55s\n",
      "epoch 319| loss: 0.08518 | val_0_accuracy: 0.94107 |  0:12:58s\n",
      "epoch 320| loss: 0.0967  | val_0_accuracy: 0.93593 |  0:13:00s\n",
      "epoch 321| loss: 0.09901 | val_0_accuracy: 0.9335  |  0:13:02s\n",
      "epoch 322| loss: 0.09183 | val_0_accuracy: 0.93458 |  0:13:05s\n",
      "epoch 323| loss: 0.08636 | val_0_accuracy: 0.9389  |  0:13:07s\n",
      "epoch 324| loss: 0.08612 | val_0_accuracy: 0.93701 |  0:13:10s\n",
      "epoch 325| loss: 0.08697 | val_0_accuracy: 0.93728 |  0:13:12s\n",
      "epoch 326| loss: 0.09343 | val_0_accuracy: 0.94079 |  0:13:14s\n",
      "epoch 327| loss: 0.08821 | val_0_accuracy: 0.93728 |  0:13:17s\n",
      "epoch 328| loss: 0.09036 | val_0_accuracy: 0.93458 |  0:13:19s\n",
      "epoch 329| loss: 0.08952 | val_0_accuracy: 0.93809 |  0:13:22s\n",
      "epoch 330| loss: 0.08572 | val_0_accuracy: 0.93512 |  0:13:25s\n",
      "epoch 331| loss: 0.08658 | val_0_accuracy: 0.93674 |  0:13:28s\n",
      "epoch 332| loss: 0.09453 | val_0_accuracy: 0.93998 |  0:13:31s\n",
      "epoch 333| loss: 0.09352 | val_0_accuracy: 0.93431 |  0:13:34s\n",
      "epoch 334| loss: 0.08681 | val_0_accuracy: 0.94025 |  0:13:36s\n",
      "epoch 335| loss: 0.08777 | val_0_accuracy: 0.93647 |  0:13:39s\n",
      "epoch 336| loss: 0.08642 | val_0_accuracy: 0.93782 |  0:13:41s\n",
      "epoch 337| loss: 0.08669 | val_0_accuracy: 0.93701 |  0:13:43s\n",
      "epoch 338| loss: 0.08586 | val_0_accuracy: 0.93809 |  0:13:45s\n",
      "epoch 339| loss: 0.08587 | val_0_accuracy: 0.93782 |  0:13:48s\n",
      "epoch 340| loss: 0.08385 | val_0_accuracy: 0.93701 |  0:13:50s\n",
      "epoch 341| loss: 0.08901 | val_0_accuracy: 0.93674 |  0:13:53s\n",
      "epoch 342| loss: 0.0847  | val_0_accuracy: 0.93944 |  0:13:56s\n",
      "epoch 343| loss: 0.09341 | val_0_accuracy: 0.93106 |  0:13:58s\n",
      "epoch 344| loss: 0.08995 | val_0_accuracy: 0.93917 |  0:14:01s\n",
      "epoch 345| loss: 0.0865  | val_0_accuracy: 0.93566 |  0:14:03s\n",
      "epoch 346| loss: 0.08913 | val_0_accuracy: 0.9362  |  0:14:06s\n",
      "epoch 347| loss: 0.08865 | val_0_accuracy: 0.93539 |  0:14:08s\n",
      "epoch 348| loss: 0.08792 | val_0_accuracy: 0.93755 |  0:14:11s\n",
      "epoch 349| loss: 0.09858 | val_0_accuracy: 0.9362  |  0:14:14s\n",
      "epoch 350| loss: 0.08995 | val_0_accuracy: 0.93971 |  0:14:16s\n",
      "epoch 351| loss: 0.08653 | val_0_accuracy: 0.93187 |  0:14:18s\n",
      "epoch 352| loss: 0.08922 | val_0_accuracy: 0.93701 |  0:14:21s\n",
      "epoch 353| loss: 0.0882  | val_0_accuracy: 0.94052 |  0:14:23s\n",
      "epoch 354| loss: 0.0926  | val_0_accuracy: 0.93809 |  0:14:26s\n",
      "epoch 355| loss: 0.09064 | val_0_accuracy: 0.93755 |  0:14:28s\n",
      "epoch 356| loss: 0.08279 | val_0_accuracy: 0.93944 |  0:14:30s\n",
      "epoch 357| loss: 0.08265 | val_0_accuracy: 0.94323 |  0:14:33s\n",
      "epoch 358| loss: 0.09084 | val_0_accuracy: 0.92106 |  0:14:35s\n",
      "epoch 359| loss: 0.09724 | val_0_accuracy: 0.93809 |  0:14:37s\n",
      "epoch 360| loss: 0.08295 | val_0_accuracy: 0.93593 |  0:14:40s\n",
      "epoch 361| loss: 0.08328 | val_0_accuracy: 0.92998 |  0:14:42s\n",
      "epoch 362| loss: 0.08249 | val_0_accuracy: 0.93593 |  0:14:45s\n",
      "epoch 363| loss: 0.08081 | val_0_accuracy: 0.93566 |  0:14:47s\n",
      "epoch 364| loss: 0.08941 | val_0_accuracy: 0.93295 |  0:14:49s\n",
      "epoch 365| loss: 0.09043 | val_0_accuracy: 0.93728 |  0:14:52s\n",
      "epoch 366| loss: 0.09219 | val_0_accuracy: 0.94079 |  0:14:54s\n",
      "epoch 367| loss: 0.08398 | val_0_accuracy: 0.93701 |  0:14:56s\n",
      "epoch 368| loss: 0.08227 | val_0_accuracy: 0.93755 |  0:14:59s\n",
      "epoch 369| loss: 0.08741 | val_0_accuracy: 0.93485 |  0:15:01s\n",
      "epoch 370| loss: 0.08584 | val_0_accuracy: 0.93323 |  0:15:04s\n",
      "epoch 371| loss: 0.0832  | val_0_accuracy: 0.93593 |  0:15:06s\n",
      "epoch 372| loss: 0.08271 | val_0_accuracy: 0.94052 |  0:15:08s\n",
      "epoch 373| loss: 0.08973 | val_0_accuracy: 0.93566 |  0:15:11s\n",
      "epoch 374| loss: 0.08904 | val_0_accuracy: 0.93917 |  0:15:13s\n",
      "epoch 375| loss: 0.08292 | val_0_accuracy: 0.93728 |  0:15:16s\n",
      "epoch 376| loss: 0.08779 | val_0_accuracy: 0.93458 |  0:15:18s\n",
      "epoch 377| loss: 0.08633 | val_0_accuracy: 0.93241 |  0:15:21s\n",
      "epoch 378| loss: 0.08369 | val_0_accuracy: 0.93701 |  0:15:23s\n",
      "epoch 379| loss: 0.0843  | val_0_accuracy: 0.93944 |  0:15:25s\n",
      "epoch 380| loss: 0.08427 | val_0_accuracy: 0.94134 |  0:15:28s\n",
      "epoch 381| loss: 0.08522 | val_0_accuracy: 0.93512 |  0:15:30s\n",
      "epoch 382| loss: 0.08657 | val_0_accuracy: 0.94269 |  0:15:32s\n",
      "epoch 383| loss: 0.08518 | val_0_accuracy: 0.9362  |  0:15:35s\n",
      "epoch 384| loss: 0.08622 | val_0_accuracy: 0.94107 |  0:15:37s\n",
      "epoch 385| loss: 0.08375 | val_0_accuracy: 0.93836 |  0:15:40s\n",
      "epoch 386| loss: 0.08123 | val_0_accuracy: 0.93998 |  0:15:42s\n",
      "epoch 387| loss: 0.08742 | val_0_accuracy: 0.93782 |  0:15:44s\n",
      "epoch 388| loss: 0.08408 | val_0_accuracy: 0.93485 |  0:15:47s\n",
      "epoch 389| loss: 0.08105 | val_0_accuracy: 0.93944 |  0:15:49s\n",
      "epoch 390| loss: 0.08568 | val_0_accuracy: 0.93809 |  0:15:51s\n",
      "epoch 391| loss: 0.08246 | val_0_accuracy: 0.94052 |  0:15:54s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 392| loss: 0.08282 | val_0_accuracy: 0.93971 |  0:15:56s\n",
      "epoch 393| loss: 0.08791 | val_0_accuracy: 0.9335  |  0:15:59s\n",
      "epoch 394| loss: 0.08559 | val_0_accuracy: 0.93512 |  0:16:01s\n",
      "epoch 395| loss: 0.08255 | val_0_accuracy: 0.9389  |  0:16:03s\n",
      "epoch 396| loss: 0.08187 | val_0_accuracy: 0.93268 |  0:16:06s\n",
      "epoch 397| loss: 0.08334 | val_0_accuracy: 0.93404 |  0:16:08s\n",
      "epoch 398| loss: 0.08491 | val_0_accuracy: 0.93512 |  0:16:11s\n",
      "epoch 399| loss: 0.08508 | val_0_accuracy: 0.93863 |  0:16:13s\n",
      "epoch 400| loss: 0.0789  | val_0_accuracy: 0.93647 |  0:16:16s\n",
      "epoch 401| loss: 0.08532 | val_0_accuracy: 0.93782 |  0:16:18s\n",
      "epoch 402| loss: 0.08269 | val_0_accuracy: 0.93214 |  0:16:21s\n",
      "epoch 403| loss: 0.08321 | val_0_accuracy: 0.93539 |  0:16:23s\n",
      "epoch 404| loss: 0.08171 | val_0_accuracy: 0.93593 |  0:16:25s\n",
      "epoch 405| loss: 0.08504 | val_0_accuracy: 0.93728 |  0:16:28s\n",
      "epoch 406| loss: 0.0865  | val_0_accuracy: 0.93485 |  0:16:30s\n",
      "epoch 407| loss: 0.08895 | val_0_accuracy: 0.93809 |  0:16:33s\n",
      "epoch 408| loss: 0.08438 | val_0_accuracy: 0.94025 |  0:16:35s\n",
      "epoch 409| loss: 0.08672 | val_0_accuracy: 0.93512 |  0:16:38s\n",
      "epoch 410| loss: 0.08276 | val_0_accuracy: 0.9335  |  0:16:40s\n",
      "epoch 411| loss: 0.0851  | val_0_accuracy: 0.93755 |  0:16:43s\n",
      "epoch 412| loss: 0.08027 | val_0_accuracy: 0.93268 |  0:16:45s\n",
      "epoch 413| loss: 0.08562 | val_0_accuracy: 0.93863 |  0:16:48s\n",
      "epoch 414| loss: 0.08469 | val_0_accuracy: 0.94107 |  0:16:50s\n",
      "epoch 415| loss: 0.08392 | val_0_accuracy: 0.93485 |  0:16:53s\n",
      "epoch 416| loss: 0.08478 | val_0_accuracy: 0.93917 |  0:16:55s\n",
      "epoch 417| loss: 0.08102 | val_0_accuracy: 0.93944 |  0:16:58s\n",
      "epoch 418| loss: 0.08049 | val_0_accuracy: 0.93782 |  0:17:00s\n",
      "epoch 419| loss: 0.08725 | val_0_accuracy: 0.9335  |  0:17:03s\n",
      "epoch 420| loss: 0.08196 | val_0_accuracy: 0.93701 |  0:17:05s\n",
      "epoch 421| loss: 0.07935 | val_0_accuracy: 0.93782 |  0:17:08s\n",
      "epoch 422| loss: 0.08751 | val_0_accuracy: 0.93485 |  0:17:11s\n",
      "epoch 423| loss: 0.08597 | val_0_accuracy: 0.93025 |  0:17:13s\n",
      "epoch 424| loss: 0.08662 | val_0_accuracy: 0.93809 |  0:17:16s\n",
      "epoch 425| loss: 0.08339 | val_0_accuracy: 0.93241 |  0:17:18s\n",
      "epoch 426| loss: 0.08436 | val_0_accuracy: 0.93214 |  0:17:20s\n",
      "epoch 427| loss: 0.07953 | val_0_accuracy: 0.93863 |  0:17:23s\n",
      "epoch 428| loss: 0.08025 | val_0_accuracy: 0.93485 |  0:17:25s\n",
      "epoch 429| loss: 0.0851  | val_0_accuracy: 0.93539 |  0:17:28s\n",
      "epoch 430| loss: 0.08393 | val_0_accuracy: 0.93458 |  0:17:30s\n",
      "epoch 431| loss: 0.07839 | val_0_accuracy: 0.93944 |  0:17:32s\n",
      "epoch 432| loss: 0.08455 | val_0_accuracy: 0.94052 |  0:17:35s\n",
      "epoch 433| loss: 0.07993 | val_0_accuracy: 0.93755 |  0:17:37s\n",
      "epoch 434| loss: 0.08035 | val_0_accuracy: 0.93701 |  0:17:40s\n",
      "epoch 435| loss: 0.08157 | val_0_accuracy: 0.93566 |  0:17:42s\n",
      "epoch 436| loss: 0.08362 | val_0_accuracy: 0.94134 |  0:17:44s\n",
      "epoch 437| loss: 0.08628 | val_0_accuracy: 0.93863 |  0:17:47s\n",
      "epoch 438| loss: 0.08463 | val_0_accuracy: 0.93755 |  0:17:49s\n",
      "epoch 439| loss: 0.08261 | val_0_accuracy: 0.93728 |  0:17:52s\n",
      "epoch 440| loss: 0.10163 | val_0_accuracy: 0.93485 |  0:17:54s\n",
      "epoch 441| loss: 0.09065 | val_0_accuracy: 0.9362  |  0:17:56s\n",
      "epoch 442| loss: 0.08646 | val_0_accuracy: 0.9362  |  0:17:59s\n",
      "epoch 443| loss: 0.08363 | val_0_accuracy: 0.93917 |  0:18:01s\n",
      "epoch 444| loss: 0.08502 | val_0_accuracy: 0.9362  |  0:18:04s\n",
      "epoch 445| loss: 0.0817  | val_0_accuracy: 0.93295 |  0:18:06s\n",
      "epoch 446| loss: 0.08295 | val_0_accuracy: 0.93674 |  0:18:08s\n",
      "epoch 447| loss: 0.08098 | val_0_accuracy: 0.93458 |  0:18:11s\n",
      "epoch 448| loss: 0.0833  | val_0_accuracy: 0.94025 |  0:18:13s\n",
      "epoch 449| loss: 0.08576 | val_0_accuracy: 0.93241 |  0:18:15s\n",
      "epoch 450| loss: 0.08391 | val_0_accuracy: 0.93458 |  0:18:18s\n",
      "epoch 451| loss: 0.07929 | val_0_accuracy: 0.93323 |  0:18:20s\n",
      "epoch 452| loss: 0.08034 | val_0_accuracy: 0.93566 |  0:18:23s\n",
      "epoch 453| loss: 0.10252 | val_0_accuracy: 0.93593 |  0:18:25s\n",
      "epoch 454| loss: 0.09671 | val_0_accuracy: 0.93566 |  0:18:27s\n",
      "epoch 455| loss: 0.09301 | val_0_accuracy: 0.9316  |  0:18:30s\n",
      "epoch 456| loss: 0.09053 | val_0_accuracy: 0.93836 |  0:18:32s\n",
      "epoch 457| loss: 0.08519 | val_0_accuracy: 0.93728 |  0:18:35s\n",
      "epoch 458| loss: 0.0832  | val_0_accuracy: 0.9362  |  0:18:37s\n",
      "epoch 459| loss: 0.07729 | val_0_accuracy: 0.93836 |  0:18:40s\n",
      "epoch 460| loss: 0.07829 | val_0_accuracy: 0.93377 |  0:18:42s\n",
      "epoch 461| loss: 0.08094 | val_0_accuracy: 0.93647 |  0:18:45s\n",
      "epoch 462| loss: 0.07472 | val_0_accuracy: 0.93755 |  0:18:47s\n",
      "epoch 463| loss: 0.08339 | val_0_accuracy: 0.9389  |  0:18:50s\n",
      "epoch 464| loss: 0.07711 | val_0_accuracy: 0.93809 |  0:18:52s\n",
      "epoch 465| loss: 0.0777  | val_0_accuracy: 0.93782 |  0:18:55s\n",
      "epoch 466| loss: 0.07505 | val_0_accuracy: 0.93431 |  0:18:57s\n",
      "epoch 467| loss: 0.08248 | val_0_accuracy: 0.93998 |  0:18:59s\n",
      "epoch 468| loss: 0.08476 | val_0_accuracy: 0.93431 |  0:19:02s\n",
      "epoch 469| loss: 0.07832 | val_0_accuracy: 0.92998 |  0:19:04s\n",
      "epoch 470| loss: 0.08238 | val_0_accuracy: 0.93674 |  0:19:07s\n",
      "epoch 471| loss: 0.07909 | val_0_accuracy: 0.9362  |  0:19:09s\n",
      "epoch 472| loss: 0.07588 | val_0_accuracy: 0.93323 |  0:19:12s\n",
      "epoch 473| loss: 0.07668 | val_0_accuracy: 0.93079 |  0:19:14s\n",
      "epoch 474| loss: 0.07962 | val_0_accuracy: 0.93512 |  0:19:16s\n",
      "epoch 475| loss: 0.09041 | val_0_accuracy: 0.93593 |  0:19:19s\n",
      "epoch 476| loss: 0.08575 | val_0_accuracy: 0.93241 |  0:19:22s\n",
      "epoch 477| loss: 0.08348 | val_0_accuracy: 0.93295 |  0:19:25s\n",
      "epoch 478| loss: 0.08114 | val_0_accuracy: 0.9362  |  0:19:27s\n",
      "epoch 479| loss: 0.07849 | val_0_accuracy: 0.94107 |  0:19:30s\n",
      "epoch 480| loss: 0.09321 | val_0_accuracy: 0.9362  |  0:19:32s\n",
      "epoch 481| loss: 0.09247 | val_0_accuracy: 0.93863 |  0:19:35s\n",
      "epoch 482| loss: 0.08679 | val_0_accuracy: 0.93701 |  0:19:37s\n",
      "epoch 483| loss: 0.08105 | val_0_accuracy: 0.93836 |  0:19:40s\n",
      "epoch 484| loss: 0.08502 | val_0_accuracy: 0.92971 |  0:19:42s\n",
      "epoch 485| loss: 0.08113 | val_0_accuracy: 0.93755 |  0:19:45s\n",
      "epoch 486| loss: 0.077   | val_0_accuracy: 0.93647 |  0:19:48s\n",
      "epoch 487| loss: 0.07747 | val_0_accuracy: 0.93593 |  0:19:50s\n",
      "epoch 488| loss: 0.08271 | val_0_accuracy: 0.9362  |  0:19:53s\n",
      "epoch 489| loss: 0.07698 | val_0_accuracy: 0.93755 |  0:19:56s\n",
      "epoch 490| loss: 0.08061 | val_0_accuracy: 0.93025 |  0:19:58s\n",
      "epoch 491| loss: 0.08202 | val_0_accuracy: 0.93863 |  0:20:01s\n",
      "epoch 492| loss: 0.08737 | val_0_accuracy: 0.92079 |  0:20:04s\n",
      "epoch 493| loss: 0.10635 | val_0_accuracy: 0.94161 |  0:20:06s\n",
      "epoch 494| loss: 0.09261 | val_0_accuracy: 0.93836 |  0:20:08s\n",
      "epoch 495| loss: 0.08289 | val_0_accuracy: 0.93971 |  0:20:11s\n",
      "epoch 496| loss: 0.08424 | val_0_accuracy: 0.93485 |  0:20:13s\n",
      "epoch 497| loss: 0.07997 | val_0_accuracy: 0.9389  |  0:20:15s\n",
      "epoch 498| loss: 0.07843 | val_0_accuracy: 0.93971 |  0:20:18s\n",
      "epoch 499| loss: 0.0822  | val_0_accuracy: 0.93944 |  0:20:20s\n",
      "epoch 500| loss: 0.0767  | val_0_accuracy: 0.9362  |  0:20:22s\n",
      "epoch 501| loss: 0.07408 | val_0_accuracy: 0.93377 |  0:20:25s\n",
      "epoch 502| loss: 0.08071 | val_0_accuracy: 0.9362  |  0:20:27s\n",
      "epoch 503| loss: 0.08139 | val_0_accuracy: 0.93566 |  0:20:30s\n",
      "epoch 504| loss: 0.07735 | val_0_accuracy: 0.93539 |  0:20:32s\n",
      "epoch 505| loss: 0.07615 | val_0_accuracy: 0.93512 |  0:20:34s\n",
      "epoch 506| loss: 0.07938 | val_0_accuracy: 0.93809 |  0:20:37s\n",
      "epoch 507| loss: 0.07944 | val_0_accuracy: 0.93944 |  0:20:39s\n",
      "epoch 508| loss: 0.07752 | val_0_accuracy: 0.93674 |  0:20:42s\n",
      "epoch 509| loss: 0.08172 | val_0_accuracy: 0.93755 |  0:20:45s\n",
      "epoch 510| loss: 0.07699 | val_0_accuracy: 0.93701 |  0:20:47s\n",
      "epoch 511| loss: 0.07822 | val_0_accuracy: 0.93836 |  0:20:49s\n",
      "epoch 512| loss: 0.08183 | val_0_accuracy: 0.93539 |  0:20:52s\n",
      "epoch 513| loss: 0.08309 | val_0_accuracy: 0.93836 |  0:20:55s\n",
      "epoch 514| loss: 0.07848 | val_0_accuracy: 0.9362  |  0:20:57s\n",
      "epoch 515| loss: 0.0861  | val_0_accuracy: 0.93133 |  0:20:59s\n",
      "epoch 516| loss: 0.08525 | val_0_accuracy: 0.93701 |  0:21:02s\n",
      "epoch 517| loss: 0.07923 | val_0_accuracy: 0.93863 |  0:21:04s\n",
      "epoch 518| loss: 0.07701 | val_0_accuracy: 0.94052 |  0:21:06s\n",
      "epoch 519| loss: 0.0823  | val_0_accuracy: 0.93458 |  0:21:09s\n",
      "epoch 520| loss: 0.08253 | val_0_accuracy: 0.93647 |  0:21:11s\n",
      "epoch 521| loss: 0.07701 | val_0_accuracy: 0.93377 |  0:21:13s\n",
      "epoch 522| loss: 0.07577 | val_0_accuracy: 0.93701 |  0:21:16s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 523| loss: 0.07302 | val_0_accuracy: 0.9335  |  0:21:18s\n",
      "epoch 524| loss: 0.08018 | val_0_accuracy: 0.93728 |  0:21:20s\n",
      "epoch 525| loss: 0.07838 | val_0_accuracy: 0.93052 |  0:21:23s\n",
      "epoch 526| loss: 0.08119 | val_0_accuracy: 0.93214 |  0:21:25s\n",
      "epoch 527| loss: 0.08592 | val_0_accuracy: 0.93133 |  0:21:28s\n",
      "epoch 528| loss: 0.08268 | val_0_accuracy: 0.93701 |  0:21:30s\n",
      "epoch 529| loss: 0.08308 | val_0_accuracy: 0.9335  |  0:21:32s\n",
      "epoch 530| loss: 0.08433 | val_0_accuracy: 0.93214 |  0:21:35s\n",
      "epoch 531| loss: 0.07985 | val_0_accuracy: 0.9389  |  0:21:38s\n",
      "epoch 532| loss: 0.07478 | val_0_accuracy: 0.9362  |  0:21:40s\n",
      "epoch 533| loss: 0.07587 | val_0_accuracy: 0.9389  |  0:21:43s\n",
      "epoch 534| loss: 0.07547 | val_0_accuracy: 0.93593 |  0:21:45s\n",
      "epoch 535| loss: 0.07465 | val_0_accuracy: 0.93566 |  0:21:48s\n",
      "epoch 536| loss: 0.075   | val_0_accuracy: 0.9362  |  0:21:50s\n",
      "epoch 537| loss: 0.07703 | val_0_accuracy: 0.9362  |  0:21:52s\n",
      "epoch 538| loss: 0.0799  | val_0_accuracy: 0.93295 |  0:21:55s\n",
      "epoch 539| loss: 0.07686 | val_0_accuracy: 0.93404 |  0:21:57s\n",
      "epoch 540| loss: 0.07687 | val_0_accuracy: 0.93458 |  0:21:59s\n",
      "epoch 541| loss: 0.07588 | val_0_accuracy: 0.93458 |  0:22:02s\n",
      "epoch 542| loss: 0.07709 | val_0_accuracy: 0.92971 |  0:22:04s\n",
      "epoch 543| loss: 0.08015 | val_0_accuracy: 0.93701 |  0:22:06s\n",
      "epoch 544| loss: 0.07585 | val_0_accuracy: 0.94052 |  0:22:09s\n",
      "epoch 545| loss: 0.07691 | val_0_accuracy: 0.93323 |  0:22:11s\n",
      "epoch 546| loss: 0.08025 | val_0_accuracy: 0.93917 |  0:22:14s\n",
      "epoch 547| loss: 0.07623 | val_0_accuracy: 0.9389  |  0:22:16s\n",
      "epoch 548| loss: 0.07716 | val_0_accuracy: 0.93106 |  0:22:18s\n",
      "epoch 549| loss: 0.07618 | val_0_accuracy: 0.93187 |  0:22:21s\n",
      "epoch 550| loss: 0.07434 | val_0_accuracy: 0.93701 |  0:22:23s\n",
      "epoch 551| loss: 0.07719 | val_0_accuracy: 0.93674 |  0:22:26s\n",
      "epoch 552| loss: 0.07583 | val_0_accuracy: 0.93485 |  0:22:28s\n",
      "epoch 553| loss: 0.08173 | val_0_accuracy: 0.93566 |  0:22:30s\n",
      "epoch 554| loss: 0.07913 | val_0_accuracy: 0.93539 |  0:22:33s\n",
      "epoch 555| loss: 0.07803 | val_0_accuracy: 0.93539 |  0:22:35s\n",
      "epoch 556| loss: 0.07709 | val_0_accuracy: 0.93241 |  0:22:37s\n",
      "epoch 557| loss: 0.07344 | val_0_accuracy: 0.93809 |  0:22:40s\n",
      "epoch 558| loss: 0.07677 | val_0_accuracy: 0.93512 |  0:22:42s\n",
      "epoch 559| loss: 0.08153 | val_0_accuracy: 0.93539 |  0:22:45s\n",
      "epoch 560| loss: 0.07936 | val_0_accuracy: 0.93485 |  0:22:47s\n",
      "epoch 561| loss: 0.0743  | val_0_accuracy: 0.93241 |  0:22:49s\n",
      "epoch 562| loss: 0.07369 | val_0_accuracy: 0.93485 |  0:22:52s\n",
      "epoch 563| loss: 0.07987 | val_0_accuracy: 0.93512 |  0:22:54s\n",
      "epoch 564| loss: 0.0751  | val_0_accuracy: 0.94134 |  0:22:56s\n",
      "epoch 565| loss: 0.07419 | val_0_accuracy: 0.9289  |  0:22:59s\n",
      "epoch 566| loss: 0.07403 | val_0_accuracy: 0.93512 |  0:23:01s\n",
      "epoch 567| loss: 0.07319 | val_0_accuracy: 0.93187 |  0:23:03s\n",
      "epoch 568| loss: 0.07414 | val_0_accuracy: 0.93431 |  0:23:06s\n",
      "epoch 569| loss: 0.07426 | val_0_accuracy: 0.93404 |  0:23:08s\n",
      "epoch 570| loss: 0.07579 | val_0_accuracy: 0.93295 |  0:23:10s\n",
      "epoch 571| loss: 0.0731  | val_0_accuracy: 0.93512 |  0:23:13s\n",
      "epoch 572| loss: 0.07553 | val_0_accuracy: 0.93431 |  0:23:15s\n",
      "epoch 573| loss: 0.09759 | val_0_accuracy: 0.93268 |  0:23:17s\n",
      "epoch 574| loss: 0.08347 | val_0_accuracy: 0.93512 |  0:23:20s\n",
      "epoch 575| loss: 0.07841 | val_0_accuracy: 0.93268 |  0:23:22s\n",
      "epoch 576| loss: 0.07875 | val_0_accuracy: 0.9362  |  0:23:24s\n",
      "epoch 577| loss: 0.07396 | val_0_accuracy: 0.93647 |  0:23:27s\n",
      "epoch 578| loss: 0.07449 | val_0_accuracy: 0.93701 |  0:23:29s\n",
      "epoch 579| loss: 0.07309 | val_0_accuracy: 0.93917 |  0:23:32s\n",
      "epoch 580| loss: 0.07252 | val_0_accuracy: 0.93539 |  0:23:34s\n",
      "epoch 581| loss: 0.07713 | val_0_accuracy: 0.93701 |  0:23:36s\n",
      "epoch 582| loss: 0.07573 | val_0_accuracy: 0.93836 |  0:23:39s\n",
      "epoch 583| loss: 0.0744  | val_0_accuracy: 0.93566 |  0:23:41s\n",
      "epoch 584| loss: 0.07881 | val_0_accuracy: 0.93809 |  0:23:44s\n",
      "epoch 585| loss: 0.07404 | val_0_accuracy: 0.93782 |  0:23:46s\n",
      "epoch 586| loss: 0.07277 | val_0_accuracy: 0.93268 |  0:23:49s\n",
      "epoch 587| loss: 0.07912 | val_0_accuracy: 0.92836 |  0:23:51s\n",
      "epoch 588| loss: 0.07677 | val_0_accuracy: 0.93593 |  0:23:54s\n",
      "epoch 589| loss: 0.07452 | val_0_accuracy: 0.92971 |  0:23:57s\n",
      "epoch 590| loss: 0.08168 | val_0_accuracy: 0.93593 |  0:23:59s\n",
      "epoch 591| loss: 0.07356 | val_0_accuracy: 0.9362  |  0:24:02s\n",
      "epoch 592| loss: 0.08297 | val_0_accuracy: 0.93512 |  0:24:04s\n",
      "epoch 593| loss: 0.07422 | val_0_accuracy: 0.94134 |  0:24:06s\n",
      "epoch 594| loss: 0.08988 | val_0_accuracy: 0.93539 |  0:24:09s\n",
      "epoch 595| loss: 0.08077 | val_0_accuracy: 0.93836 |  0:24:11s\n",
      "epoch 596| loss: 0.07496 | val_0_accuracy: 0.93539 |  0:24:13s\n",
      "epoch 597| loss: 0.08031 | val_0_accuracy: 0.9362  |  0:24:16s\n",
      "epoch 598| loss: 0.07929 | val_0_accuracy: 0.93295 |  0:24:18s\n",
      "epoch 599| loss: 0.0819  | val_0_accuracy: 0.93701 |  0:24:21s\n",
      "epoch 600| loss: 0.07686 | val_0_accuracy: 0.93647 |  0:24:23s\n",
      "epoch 601| loss: 0.07269 | val_0_accuracy: 0.93214 |  0:24:25s\n",
      "epoch 602| loss: 0.07412 | val_0_accuracy: 0.9389  |  0:24:28s\n",
      "epoch 603| loss: 0.07532 | val_0_accuracy: 0.93566 |  0:24:30s\n",
      "epoch 604| loss: 0.07405 | val_0_accuracy: 0.93782 |  0:24:33s\n",
      "epoch 605| loss: 0.07169 | val_0_accuracy: 0.93485 |  0:24:35s\n",
      "epoch 606| loss: 0.07709 | val_0_accuracy: 0.93404 |  0:24:37s\n",
      "epoch 607| loss: 0.0743  | val_0_accuracy: 0.92998 |  0:24:40s\n",
      "epoch 608| loss: 0.072   | val_0_accuracy: 0.93782 |  0:24:43s\n",
      "epoch 609| loss: 0.07335 | val_0_accuracy: 0.93404 |  0:24:45s\n",
      "epoch 610| loss: 0.07391 | val_0_accuracy: 0.93539 |  0:24:48s\n",
      "epoch 611| loss: 0.07201 | val_0_accuracy: 0.93674 |  0:24:50s\n",
      "epoch 612| loss: 0.07462 | val_0_accuracy: 0.93755 |  0:24:52s\n",
      "epoch 613| loss: 0.07736 | val_0_accuracy: 0.93458 |  0:24:55s\n",
      "epoch 614| loss: 0.07646 | val_0_accuracy: 0.93782 |  0:24:57s\n",
      "epoch 615| loss: 0.0746  | val_0_accuracy: 0.93404 |  0:24:59s\n",
      "epoch 616| loss: 0.07861 | val_0_accuracy: 0.9362  |  0:25:02s\n",
      "epoch 617| loss: 0.07708 | val_0_accuracy: 0.9335  |  0:25:04s\n",
      "epoch 618| loss: 0.076   | val_0_accuracy: 0.93512 |  0:25:07s\n",
      "epoch 619| loss: 0.07289 | val_0_accuracy: 0.93674 |  0:25:09s\n",
      "epoch 620| loss: 0.07875 | val_0_accuracy: 0.93809 |  0:25:11s\n",
      "epoch 621| loss: 0.07417 | val_0_accuracy: 0.9362  |  0:25:14s\n",
      "epoch 622| loss: 0.07335 | val_0_accuracy: 0.93728 |  0:25:16s\n",
      "epoch 623| loss: 0.07808 | val_0_accuracy: 0.93106 |  0:25:19s\n",
      "epoch 624| loss: 0.07555 | val_0_accuracy: 0.9362  |  0:25:21s\n",
      "epoch 625| loss: 0.0769  | val_0_accuracy: 0.93836 |  0:25:24s\n",
      "epoch 626| loss: 0.07338 | val_0_accuracy: 0.93647 |  0:25:26s\n",
      "epoch 627| loss: 0.07713 | val_0_accuracy: 0.93971 |  0:25:29s\n",
      "epoch 628| loss: 0.0713  | val_0_accuracy: 0.93782 |  0:25:32s\n",
      "epoch 629| loss: 0.07377 | val_0_accuracy: 0.93106 |  0:25:34s\n",
      "epoch 630| loss: 0.07324 | val_0_accuracy: 0.93295 |  0:25:37s\n",
      "epoch 631| loss: 0.07504 | val_0_accuracy: 0.93512 |  0:25:40s\n",
      "epoch 632| loss: 0.07208 | val_0_accuracy: 0.93782 |  0:25:42s\n",
      "epoch 633| loss: 0.0779  | val_0_accuracy: 0.9362  |  0:25:44s\n",
      "epoch 634| loss: 0.07511 | val_0_accuracy: 0.93268 |  0:25:46s\n",
      "epoch 635| loss: 0.07564 | val_0_accuracy: 0.93755 |  0:25:49s\n",
      "epoch 636| loss: 0.07724 | val_0_accuracy: 0.93755 |  0:25:52s\n",
      "epoch 637| loss: 0.07317 | val_0_accuracy: 0.93944 |  0:25:54s\n",
      "epoch 638| loss: 0.07016 | val_0_accuracy: 0.94161 |  0:25:56s\n",
      "epoch 639| loss: 0.07384 | val_0_accuracy: 0.94107 |  0:25:59s\n",
      "epoch 640| loss: 0.07418 | val_0_accuracy: 0.93593 |  0:26:01s\n",
      "epoch 641| loss: 0.07197 | val_0_accuracy: 0.93052 |  0:26:04s\n",
      "epoch 642| loss: 0.07107 | val_0_accuracy: 0.93728 |  0:26:06s\n",
      "epoch 643| loss: 0.07349 | val_0_accuracy: 0.93512 |  0:26:09s\n",
      "epoch 644| loss: 0.0694  | val_0_accuracy: 0.93755 |  0:26:12s\n",
      "epoch 645| loss: 0.07144 | val_0_accuracy: 0.93431 |  0:26:14s\n",
      "epoch 646| loss: 0.0703  | val_0_accuracy: 0.93485 |  0:26:16s\n",
      "epoch 647| loss: 0.0761  | val_0_accuracy: 0.93323 |  0:26:19s\n",
      "epoch 648| loss: 0.07667 | val_0_accuracy: 0.93512 |  0:26:21s\n",
      "epoch 649| loss: 0.07812 | val_0_accuracy: 0.93782 |  0:26:24s\n",
      "epoch 650| loss: 0.08205 | val_0_accuracy: 0.93701 |  0:26:26s\n",
      "epoch 651| loss: 0.08127 | val_0_accuracy: 0.93809 |  0:26:29s\n",
      "epoch 652| loss: 0.07888 | val_0_accuracy: 0.93755 |  0:26:31s\n",
      "epoch 653| loss: 0.07609 | val_0_accuracy: 0.93539 |  0:26:34s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 654| loss: 0.07087 | val_0_accuracy: 0.93458 |  0:26:36s\n",
      "epoch 655| loss: 0.06907 | val_0_accuracy: 0.93566 |  0:26:39s\n",
      "epoch 656| loss: 0.07059 | val_0_accuracy: 0.93052 |  0:26:42s\n",
      "epoch 657| loss: 0.06906 | val_0_accuracy: 0.93701 |  0:26:45s\n",
      "epoch 658| loss: 0.06743 | val_0_accuracy: 0.93485 |  0:26:48s\n",
      "epoch 659| loss: 0.07496 | val_0_accuracy: 0.93728 |  0:26:50s\n",
      "epoch 660| loss: 0.07183 | val_0_accuracy: 0.93782 |  0:26:53s\n",
      "epoch 661| loss: 0.07642 | val_0_accuracy: 0.93593 |  0:26:56s\n",
      "epoch 662| loss: 0.07517 | val_0_accuracy: 0.93674 |  0:26:58s\n",
      "epoch 663| loss: 0.07268 | val_0_accuracy: 0.93647 |  0:27:01s\n",
      "epoch 664| loss: 0.07291 | val_0_accuracy: 0.93512 |  0:27:03s\n",
      "epoch 665| loss: 0.07122 | val_0_accuracy: 0.93458 |  0:27:05s\n",
      "epoch 666| loss: 0.07161 | val_0_accuracy: 0.93755 |  0:27:08s\n",
      "epoch 667| loss: 0.07192 | val_0_accuracy: 0.9389  |  0:27:10s\n",
      "epoch 668| loss: 0.07385 | val_0_accuracy: 0.93836 |  0:27:13s\n",
      "epoch 669| loss: 0.07642 | val_0_accuracy: 0.93755 |  0:27:15s\n",
      "epoch 670| loss: 0.07812 | val_0_accuracy: 0.93512 |  0:27:18s\n",
      "epoch 671| loss: 0.07521 | val_0_accuracy: 0.9362  |  0:27:21s\n",
      "epoch 672| loss: 0.07135 | val_0_accuracy: 0.9335  |  0:27:24s\n",
      "epoch 673| loss: 0.07479 | val_0_accuracy: 0.93458 |  0:27:26s\n",
      "epoch 674| loss: 0.07498 | val_0_accuracy: 0.93755 |  0:27:29s\n",
      "epoch 675| loss: 0.07005 | val_0_accuracy: 0.93187 |  0:27:31s\n",
      "epoch 676| loss: 0.07338 | val_0_accuracy: 0.93025 |  0:27:33s\n",
      "epoch 677| loss: 0.07503 | val_0_accuracy: 0.9335  |  0:27:36s\n",
      "epoch 678| loss: 0.07366 | val_0_accuracy: 0.93836 |  0:27:38s\n",
      "epoch 679| loss: 0.07618 | val_0_accuracy: 0.92863 |  0:27:40s\n",
      "epoch 680| loss: 0.07601 | val_0_accuracy: 0.93647 |  0:27:43s\n",
      "\n",
      "Early stopping occurred at epoch 680 with best_epoch = 180 and best_val_0_accuracy = 0.94404\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "clf = TabNetMultiTaskClassifier()\n",
    "\n",
    "X_train, X_valid, Y_train, y_valid = train_test_split(X, Y, test_size=0.1, stratify=Y)\n",
    "\n",
    "clf.fit(\n",
    "  X_train, Y_train,\n",
    "  eval_set=[(X_valid, y_valid)], \n",
    "  eval_metric=['accuracy'], \n",
    "  patience = 500, \n",
    "  max_epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_1000_epochs.pt.zip\n"
     ]
    }
   ],
   "source": [
    "save_path = \"tabnet_1000_epochs.pt\" \n",
    "\n",
    "saved_filepath = clf.save_model(save_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at tabnet_300_epochs.pt.zip\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "save_path = \"tabnet_300_epochs.pt\" \n",
    "\n",
    "saved_filepath = clf.save_model(save_path) \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "loaded_clf = TabNetMultiTaskClassifier()\n",
    "loaded_clf.load_model(\"tabnet_1000_epochs.pt.zip\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = processed_test[features].values \n",
    "y_test = processed_test['Target'].values \n",
    "\n",
    "preds = loaded_clf.predict(x_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.33333333333333"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds.astype(np.float)\n",
    "acc = 0 \n",
    "total = 0 \n",
    "for i in range(len(preds)):\n",
    "    if y_test[i] != 2:\n",
    "        total += 1 \n",
    "        if preds[i] == y_test[i]:\n",
    "            acc += 1 \n",
    "acc / total * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.82488479262673"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds.astype(np.float)\n",
    "acc = 0 \n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == y_test[i]:\n",
    "        acc += 1 \n",
    "\n",
    "acc / len(preds) * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision = 0.8316235715766318\n",
      "recall = 0.8129323308270676\n",
      "f1 = 0.8194428279722334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"precision = {}\".format(precision_score(y_test, preds, average='macro'))) \n",
    "print(\"recall = {}\".format(recall_score(y_test, preds, average='macro')))\n",
    "print(\"f1 = {}\".format(f1_score(y_test, preds, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 0.0\n",
      "========== correct sample! ==========\n",
      "1.0 1.0\n",
      "========== incorrect sample! =========\n",
      "2.0 1.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n",
      "========== correct sample! ==========\n",
      "0.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.33333333333333"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = preds.astype(np.float)\n",
    "acc = 0 \n",
    "total = 0 \n",
    "for i in range(len(preds)):\n",
    "    if y_test[i] != 2:\n",
    "        total += 1 \n",
    "        if preds[i] == y_test[i]:\n",
    "            print(\"========== correct sample! ==========\")\n",
    "            print(preds[i], y_test[i])\n",
    "            acc += 1 \n",
    "        else:\n",
    "            print(\"========== incorrect sample! =========\")\n",
    "            print(preds[i], y_test[i])\n",
    "acc / total * 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
