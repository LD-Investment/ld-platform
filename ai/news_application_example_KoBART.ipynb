{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoBART를 활용해서 코인뉴스 한줄 요약하는 예시를 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b054f8606547ac931561c312072960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394f28ec7caf46f3a50140579cd5923e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=111.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d45565de864efdabfe1a61ac47b5a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=682152.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cb3ecff3e245d2a6458f2e66e9aa2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BartTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c57eea0f2c4b26a6170dbc100eb6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=495659091.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch \n",
    "from transformers import * \n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "model.eval()\n",
    "\n",
    "def abstractive_summarizer(text, model, tokenizer): \n",
    "    ### tokenize text for preprocessing ### \n",
    "    raw_input_ids = tokenizer.encode(text)\n",
    "    input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id] \n",
    "\n",
    "    ### generate summary using KoBART ### \n",
    "    summary_ids = model.generate(torch.tensor([input_ids]),\n",
    "                                 max_length=256,\n",
    "                                 early_stopping=True,\n",
    "                                 repetition_penalty=2.0)\n",
    "    summ = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True) \n",
    "    return summ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "요약문:\n",
      "KB국민은행은 채용 비리에 연루된 여성 지원자를 탈락시키는 등 의도적인 남녀 차별의 정황이 드러나고 재판에 넘겨져서 1심과 2심 모두 유죄를 선고받았다.\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "2017년 은행들이 채용 적정성에 대해 금융감독원에 보고한 자체 점검 결과다. 하지만 금감원이 현장 검사를 벌인 결과는 달랐다. 현장 검사 두 달 만에 금감원은 총 22건의 채용 비리 정황을 발견했다. 당시 이미 수사 중이던 우리은행을 시작으로 KB국민·신한·하나은행 등 전 시중은행이 채용 비리에 연루됐던 것이다.\n",
    "\n",
    "이 가운데 KB국민은행은 ‘VIP 리스트’를 관리하며 남성 지원자들의 점수를 임의로 올려 여성 지원자를 탈락시키는 등 의도적인 남녀 차별의 정황 등이 드러나며 재판에 넘겨졌다.\n",
    "\n",
    "소문으로만 듣던 남녀 차별 채용이 현실로 드러난 것이다. 이에 2022년 1월 14일 대법원은 KB국민은행과 임직원에 대해 유죄를 확정지었다. 당시 KB국민은행 인사팀장이던 A 씨는 징역 1년의 실형을 선고받았다. 회사도 남녀고용평등법 위반으로 500만원의 벌금을 물게 됐다.\n",
    "\n",
    "사건은 2015년으로 거슬러 올라간다. KB국민은행 인사팀장 A 씨는 2015년 상반기 KB국민은행 신입 행원 채용 과정에서 일명 ‘VIP 리스트’를 받게 된다. 최고경영진 친인척뿐만 아니라 상사들로부터 청탁을 받은 지원자 명단이었다. 그뿐만 아니라 지시 중에는 ‘신입 행원 최종 합격자의 남성과 여성 비율을 6 대 4나 7 대 3으로 하라’는 내용도 있었다.\n",
    "\n",
    "A 씨는 남성 지원자 113명에 대해 서류 전형 평가 심사위원들이 부여한 자기소개서의 평가 등급을 임의로 상향한 반면 여성 지원자 112명은 자기소개서의 평가 등급을 임의로 하향 조정했다.\n",
    "\n",
    "또한 2차 면접 과정에서 청탁 대상자 20명을 포함해 28명의 면접 점수를 조작하고 그중 20명을 부정한 방법으로 합격시킨 혐의도 받았다. 그뿐만 아니라 2015년 하반기 신입 행원 채용과 2015∼2017년 인턴 채용 과정에서는 수백 명의 서류 전형과 면접 전형 점수를 조작하는 방법으로 청탁 대상자를 선발한 것으로 조사됐다.\n",
    "\n",
    "사건을 수사한 검찰은 A 씨를 비롯해 최종 결재권자인 전 부행장 B 씨, 인력지원부장·HR 총괄상무를 지낸 C 씨, 전 HR 본부장 D 씨 등 임직원 3명을 함께 기소했다.\n",
    "\n",
    "1심 재판부는 2018년 10월 업무 방해 등의 혐의로 기소된 A·B·C 씨에게 각 징역 1년에 집행 유예 2년을 선고했다. D 씨에게는 징역 10개월에 집행 유예 2년을 선고했다.\n",
    "\n",
    "재판부는 “불법적인 조작으로 점수가 변경돼 당락이 달라진 지원자의 규모가 상당하다”면서도 “임직원들이 경제적 이득을 취득했다고 볼 사정이 없고 잘못된 관행을 답습하는 과정에서 범행에 이르게 됐다”며 집행 유예를 내린 이유를 설명했다. 남녀고용평등법 위반 혐의를 받는 주식회사 KB국민은행에는 벌금 500만원을 선고했다.\n",
    "\n",
    "A 씨는 판결에 불복해 항소했다. 하지만 2심 재판부는 오히려 징역 1년에 집행 유예 2년을 선고한 1심을 깨고 A 씨에게 징역 1년을 선고한 뒤 법정 구속했다. 항소심 재판부는 “다른 사건과 비교해 많은 지원자의 합격 여부가 변경돼 죄질이 좋지 않은 데도 범행을 반성하지 않고 있다”고 지적했다.\n",
    "\n",
    "결국 A 씨는 대법원에 상고했지만 원심의 판단은 바뀌지 않았다. 대법원 제2부(주심 이동원 대법관)는 1월 14일 업무 방해와 남녀고용평등법 위반 혐의로 기소된 KB국민은행 전 인사팀장 A 씨에게 징역 1년을 선고한 원심을 확정했다. 다른 피고인들도 원심 형이 그대로 유지됐다.\n",
    "\n",
    "2021년 11월에는 신한은행 신입 행원 공채 과정에서 드러난 채용 비리 사건 항소심의 결론이 나왔다. 해당 재판에서도 채용 비리와 관련해 재판에 넘겨진 임직원 대부분은 유죄를 선고받았지만 유일하게 조용병 신한금융지주 회장만 1심을 깨고 무죄를 선고받았다.\n",
    "\n",
    "신한은행은 2013년 상반기부터 2015년 상반기까지 신입 사원 공채에서 부정 청탁 등을 통해 채용한 혐의로 재판에 넘겨졌다. 1심은 조 회장에게 징역 6개월에 집행 유예 2년을 선고했지만 항소심은 이를 파기하고 무죄를 선고했다.\n",
    "\n",
    "조 회장은 신한은행장으로 재직하던 2015~2016년 총 3명의 지원자 합격 과정에 부당하게 관여한 혐의로 2018년 기소됐다. 2심 재판부는 “이 중 2명은 부정 통과자로 보기 어렵다”고 설명했다. “청탁 대상자이거나 은행 임직원 자녀라고 해도 일반 지원자와 마찬가지로 채용 과정을 거치고 대학이나 어학 점수 등 기본적인 스펙을 갖추고 있다면 부정 통과자로 보기 어렵다”는 것이 이유다.\n",
    "\n",
    "재판부는 다른 한 명의 지원자에 대해서도 “조 회장이 지원자의 서류 지원을 전달한 사실만으로는 ‘합격 지시’로 간주할 수 없다”고 판단했다. 그러면서 “조 회장이 합격 지시를 내린 것이라면 채용팀이 해당 지원자의 서류 전형은 통과시키고 1차 면접에서 탈락시키지는 않았을 것으로 보인다”고 덧붙였다.\n",
    "\n",
    "조 회장과 함께 기소된 다른 인사팀 관계자들도 형량이 감경돼 벌금형이나 집행 유예를 선고받았다. 검찰은 항소심 판결을 받아들일 수 없다며 대법원 상고를 결정했다.\n",
    "\n",
    "'''\n",
    "print(\"요약문:\")\n",
    "print(abstractive_summarizer(text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trying Multilingual BART \n",
    "\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-en-ro\", src_lang=\"en_XX\", tgt_lang=\"ro_RO\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(30000, 768, padding_idx=3)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
       "      (layers): ModuleList(\n",
       "        (0): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### define KoBART model ### \n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-summarization')\n",
    "model = BartForConditionalGeneration.from_pretrained('gogamza/kobart-summarization')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문:\n",
      "\n",
      "솔라나 지갑 팬텀, 모바일 앱 내일 출시\n",
      "코인데스크에 따르면, 솔라나 지갑 팬텀(Phantom)이 30일 iOS 베타 버전을 출시하고, 한달 안에 전체 버전을 출시할 예정이다. 모바일 지갑은 암호화폐와 NFT 거래, 전송, 스왑, 스테이킹 등을 지원한다. 팬텀 지갑은 11월 100만 명의 활성 사용자를 유치, 모바일 버전 없이 솔플레어(SolFlare) 지갑 사용자 수를 제친 바 있다.\n",
      "\n",
      "\n",
      "요약문:\n",
      "30일 iOS 베타 버전을 출시하는 솔라나 지갑 팬텀(Phantom)은 암호화폐와 NFT 거래, 전송 등을 지원하는 모바일 앱 전체버전도 선보일 예정이다.\n"
     ]
    }
   ],
   "source": [
    "### text scraped from source ### \n",
    "text = '''\n",
    "솔라나 지갑 팬텀, 모바일 앱 내일 출시\n",
    "코인데스크에 따르면, 솔라나 지갑 팬텀(Phantom)이 30일 iOS 베타 버전을 출시하고, 한달 안에 전체 버전을 출시할 예정이다. 모바일 지갑은 암호화폐와 NFT 거래, 전송, 스왑, 스테이킹 등을 지원한다. 팬텀 지갑은 11월 100만 명의 활성 사용자를 유치, 모바일 버전 없이 솔플레어(SolFlare) 지갑 사용자 수를 제친 바 있다.\n",
    "'''\n",
    "\n",
    "### tokenize text for preprocessing ### \n",
    "raw_input_ids = tokenizer.encode(text)\n",
    "input_ids = [tokenizer.bos_token_id] + raw_input_ids + [tokenizer.eos_token_id] \n",
    "\n",
    "### generate summary using KoBART ### \n",
    "summary_ids = model.generate(torch.tensor([input_ids]),\n",
    "                             max_length=256,\n",
    "                             early_stopping=True,\n",
    "                             repetition_penalty=2.0)\n",
    "\n",
    "summ = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True) \n",
    "\n",
    "print(\"원문:\")\n",
    "print(text)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"요약문:\")\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
